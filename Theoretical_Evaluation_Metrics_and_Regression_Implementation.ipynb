{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f6cbe4c-bada-4d12-a427-89baf086290b",
   "metadata": {},
   "source": [
    "## Theoretical"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55394b8-f2a9-43b1-9ae0-98d4662a94f9",
   "metadata": {},
   "source": [
    "### Q1 What does R-squared represent in a regression model?\n",
    "\n",
    "R-squared, also known as the coefficient of determination, represents the proportion of the variance in the dependent variable (target) that is predictable from the independent variables (features). It provides a measure of how well the regression model fits the data. \n",
    "\n",
    "- The value of R-squared ranges between 0 and 1.\n",
    "- A value closer to 1 indicates a better fit, meaning the model explains a large portion of the variability in the target variable.\n",
    "- Conversely, a value closer to 0 suggests that the model does not explain much of the variability in the target variable.\n",
    "\n",
    "For example, an R-squared value of 0.8 means 80% of the variance in the dependent variable is explained by the independent variables, while the remaining 20% is unexplained.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ae81eb-ddde-47a8-8ddd-db354ca2ce1d",
   "metadata": {},
   "source": [
    "### Q2 What are the assumptions of linear regression?\n",
    "\n",
    "Linear regression relies on several key assumptions to ensure the validity of the model:\n",
    "\n",
    "1. **Linearity**: The relationship between the independent and dependent variables must be linear.\n",
    "2. **Independence**: The residuals (errors) must be independent of each other, meaning no autocorrelation exists.\n",
    "3. **Homoscedasticity**: The variance of residuals should be constant across all levels of the independent variables (no heteroscedasticity).\n",
    "4. **Normality**: The residuals should follow a normal distribution.\n",
    "5. **No Multicollinearity**: The independent variables should not be highly correlated with each other.\n",
    "\n",
    "Violating these assumptions can lead to biased or inefficient estimates, making the regression results unreliable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df10e53-e0ac-452c-b8af-39fa0832d04b",
   "metadata": {},
   "source": [
    "### Q3 What is the difference between R-squared and Adjusted R-squared?\n",
    "\n",
    "While both R-squared and Adjusted R-squared measure how well a regression model fits the data, there is a key difference between them:\n",
    "\n",
    "- **R-squared**:\n",
    "  - Measures the proportion of variance in the dependent variable explained by the independent variables.\n",
    "  - Does not account for the number of predictors in the model.\n",
    "  - Tends to increase as more variables are added to the model, even if they do not improve the model's predictive power.\n",
    "\n",
    "- **Adjusted R-squared**:\n",
    "  - Adjusts the R-squared value to account for the number of predictors in the model.\n",
    "  - Penalizes the addition of irrelevant predictors to avoid overfitting.\n",
    "  - Can decrease if additional predictors do not contribute significantly to the model.\n",
    "\n",
    "The formula for Adjusted R-squared is:\n",
    "\\[\n",
    "\\text{Adjusted R}^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "\\]\n",
    "where \\( n \\) is the number of observations and \\( k \\) is the number of predictors.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ace7080b-b8ba-4862-9e3b-719b043c58f1",
   "metadata": {},
   "source": [
    "### Q4 Why do we use Mean Squared Error (MSE)?\n",
    "\n",
    "Mean Squared Error (MSE) is used as a standard metric to evaluate the performance of a regression model. It calculates the average of the squared differences between the actual and predicted values:\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "Key reasons for using MSE:\n",
    "- **Penalizes large errors**: Squaring the errors emphasizes larger errors more than smaller ones, making MSE sensitive to outliers.\n",
    "- **Continuous metric**: Provides a single, continuous value that reflects the model's predictive accuracy.\n",
    "- **Easy to interpret**: Smaller MSE values indicate better model performance.\n",
    "- **Differentiability**: MSE is differentiable, which is important for optimization algorithms like gradient descent used in model training.\n",
    "\n",
    "MSE is commonly used to compare different models and assess which one performs better on the given dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb9873e-9690-43ac-9785-ca053976ea68",
   "metadata": {},
   "source": [
    "### Q5 What does an Adjusted R-squared value of 0.85 indicate?\n",
    "\n",
    "An Adjusted R-squared value of 0.85 indicates that 85% of the variance in the dependent variable is explained by the independent variables included in the model, after accounting for the number of predictors. \n",
    "\n",
    "- This value suggests that the model has a good fit and is effective in explaining the variability in the target variable.\n",
    "- The \"adjusted\" aspect means that this measure considers the number of predictors in the model, ensuring that only relevant variables contribute to the model's performance.\n",
    "- A high Adjusted R-squared value like 0.85 generally indicates a well-performing model, though it is essential to check other metrics (e.g., MSE) and validate the assumptions of regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fd296bb-7bcc-4999-a981-a146f04ac884",
   "metadata": {},
   "source": [
    "### Q6 How do we check for normality of residuals in linear regression?\n",
    "\n",
    "To check for the normality of residuals in linear regression, we can use the following methods:\n",
    "\n",
    "1. **Histogram of Residuals**:\n",
    "   - Plot a histogram of the residuals. If the residuals are normally distributed, the histogram should resemble a bell curve.\n",
    "\n",
    "2. **Q-Q Plot (Quantile-Quantile Plot)**:\n",
    "   - A Q-Q plot compares the quantiles of the residuals to the quantiles of a normal distribution. If the points lie close to the diagonal line, the residuals are approximately normal.\n",
    "\n",
    "3. **Shapiro-Wilk Test**:\n",
    "   - A statistical test that checks the null hypothesis that the data follows a normal distribution. A p-value > 0.05 indicates the residuals are likely normal.\n",
    "\n",
    "4. **Kolmogorov-Smirnov Test**:\n",
    "   - Another statistical test to check for normality. A higher p-value supports normality.\n",
    "\n",
    "5. **Skewness and Kurtosis**:\n",
    "   - Calculate the skewness (asymmetry) and kurtosis (tailedness) of the residuals. Values close to 0 for skewness and 3 for kurtosis suggest normality.\n",
    "\n",
    "Ensuring normality of residuals is crucial for the validity of hypothesis tests and confidence intervals in linear regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "760159e2-589a-4894-ba6b-750d4b0e641b",
   "metadata": {},
   "source": [
    "### Q7 What is multicollinearity, and how does it impact regression?\n",
    "\n",
    "**Multicollinearity** occurs when two or more independent variables in a regression model are highly correlated, meaning they provide redundant information. \n",
    "\n",
    "### Impacts of Multicollinearity:\n",
    "1. **Unstable Coefficients**:\n",
    "   - The estimated coefficients become sensitive to small changes in the data, leading to unreliable results.\n",
    "\n",
    "2. **Reduced Interpretability**:\n",
    "   - It becomes difficult to determine the individual effect of each independent variable on the dependent variable.\n",
    "\n",
    "3. **Inflated Variance**:\n",
    "   - The standard errors of the coefficients increase, which can make insignificant predictors appear significant.\n",
    "\n",
    "### How to Detect Multicollinearity:\n",
    "1. **Variance Inflation Factor (VIF)**:\n",
    "   - A VIF value > 5 (or sometimes > 10) indicates high multicollinearity.\n",
    "2. **Correlation Matrix**:\n",
    "   - A high correlation (e.g., > 0.8) between two variables suggests multicollinearity.\n",
    "\n",
    "### How to Handle Multicollinearity:\n",
    "- Remove highly correlated predictors.\n",
    "- Combine correlated variables (e.g., via Principal Component Analysis).\n",
    "- Use regularization techniques like Ridge or Lasso regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321a8861-a265-4487-b85a-2c9a3e0100f1",
   "metadata": {},
   "source": [
    "### Q8 What is Mean Absolute Error (MAE)?\n",
    "\n",
    "Mean Absolute Error (MAE) is a regression evaluation metric that measures the average magnitude of errors between predicted and actual values, without considering their direction:\n",
    "\n",
    "\\[\n",
    "\\text{MAE} = \\frac{1}{n} \\sum_{i=1}^{n} |y_i - \\hat{y}_i|\n",
    "\\]\n",
    "\n",
    "### Key Features:\n",
    "1. **Interpretable**:\n",
    "   - MAE represents the average absolute difference between predicted and actual values in the same unit as the target variable.\n",
    "2. **Insensitive to Outliers**:\n",
    "   - Unlike MSE, MAE does not square the errors, so it is less sensitive to large outliers.\n",
    "3. **Easy to Understand**:\n",
    "   - A lower MAE indicates better model performance.\n",
    "\n",
    "MAE is commonly used when errors of equal magnitude, regardless of sign, are equally important.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8da63-f21b-49b4-9040-f4f756893d22",
   "metadata": {},
   "source": [
    "### Q9 What are the benefits of using an ML pipeline?\n",
    "\n",
    "An ML pipeline is a structured sequence of data preprocessing, feature engineering, and modeling steps to streamline the machine learning workflow.\n",
    "\n",
    "### Benefits:\n",
    "1. **Automation**:\n",
    "   - Automates repetitive tasks like data preprocessing and model evaluation.\n",
    "\n",
    "2. **Reproducibility**:\n",
    "   - Ensures consistent results by applying the same sequence of operations.\n",
    "\n",
    "3. **Modularity**:\n",
    "   - Each step (e.g., scaling, encoding, modeling) is encapsulated, making it easy to update or debug individual components.\n",
    "\n",
    "4. **Error Reduction**:\n",
    "   - Reduces the risk of errors by handling data preprocessing and modeling in a systematic manner.\n",
    "\n",
    "5. **Efficiency**:\n",
    "   - Facilitates rapid experimentation and deployment.\n",
    "\n",
    "6. **Cross-validation Integration**:\n",
    "   - Pipelines allow seamless integration with cross-validation to assess the model’s performance.\n",
    "\n",
    "Pipelines are implemented in libraries like Scikit-learn, simplifying the ML workflow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becbe14a-43e8-49b7-b5b8-f612ed45fd7b",
   "metadata": {},
   "source": [
    "### Q10 Why is RMSE considered more interpretable than MSE?\n",
    "\n",
    "Root Mean Squared Error (RMSE) is often considered more interpretable than Mean Squared Error (MSE) because:\n",
    "\n",
    "1. **Same Units**:\n",
    "   - RMSE is expressed in the same units as the dependent variable, making it easier to interpret and compare with actual values. In contrast, MSE has squared units.\n",
    "\n",
    "2. **Direct Error Magnitude**:\n",
    "   - RMSE provides a direct measure of the average prediction error, while MSE exaggerates larger errors due to squaring.\n",
    "\n",
    "For example:\n",
    "- If RMSE = 10, the model’s average prediction error is approximately 10 units.\n",
    "- RMSE emphasizes large errors more than MAE, making it more suitable for applications where larger errors are more critical.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44af78fa-5dd5-491d-afb0-d454d9e04ea0",
   "metadata": {},
   "source": [
    "### Q11 What is pickling in Python, and how is it useful in ML?\n",
    "\n",
    "**Pickling** is the process of serializing a Python object into a byte stream, which can later be deserialized (unpickled) to reconstruct the original object.\n",
    "\n",
    "### How Pickling is Useful in ML:\n",
    "1. **Model Persistence**:\n",
    "   - Trained machine learning models can be saved to disk as a pickle file and loaded later for predictions without retraining.\n",
    "\n",
    "2. **Pipeline Saving**:\n",
    "   - Entire pipelines, including preprocessing steps and models, can be saved and reused.\n",
    "\n",
    "3. **Sharing**:\n",
    "   - Pickle files can be shared between systems or teams to replicate experiments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c5f614-e35c-4edb-9215-cb5cdcab7bd2",
   "metadata": {},
   "source": [
    "### Q12 What does a high R-squared value mean?\n",
    "\n",
    "A high R-squared value indicates that a large proportion of the variance in the dependent variable is explained by the independent variables in the regression model.\n",
    "\n",
    "### Key Points:\n",
    "1. **Good Model Fit**:\n",
    "   - A high R-squared suggests that the model fits the data well.\n",
    "2. **Context Dependent**:\n",
    "   - The significance of a high R-squared depends on the context and domain. In some fields (e.g., physical sciences), high values are common, while in others (e.g., social sciences), lower values are acceptable.\n",
    "3. **Does Not Imply Causation**:\n",
    "   - A high R-squared does not mean the predictors cause changes in the dependent variable, only that they are associated.\n",
    "\n",
    "For example, an R-squared of 0.90 means that 90% of the variance in the dependent variable is explained by the model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daae528a-33b3-4d61-90c5-db0c6b6a5966",
   "metadata": {},
   "source": [
    "### Q13 What happens if linear regression assumptions are violated?\n",
    "\n",
    "Violations of linear regression assumptions can lead to unreliable or biased results. Here’s how each violation affects the model:\n",
    "\n",
    "1. **Linearity Violation**:\n",
    "   - The model cannot capture the true relationship, leading to poor predictions and biased coefficients.\n",
    "\n",
    "2. **Independence Violation**:\n",
    "   - Residuals are correlated (e.g., in time-series data), causing incorrect standard errors and misleading hypothesis tests.\n",
    "\n",
    "3. **Homoscedasticity Violation**:\n",
    "   - Unequal variance of residuals (heteroscedasticity) can result in inefficient estimates and unreliable confidence intervals.\n",
    "\n",
    "4. **Normality Violation**:\n",
    "   - Non-normal residuals affect hypothesis testing and confidence intervals, though predictions may still be accurate.\n",
    "\n",
    "5. **Multicollinearity**:\n",
    "   - High correlation between predictors inflates standard errors, making it difficult to determine the importance of individual variables.\n",
    "\n",
    "Addressing these violations through transformations, robust methods, or alternative models can improve reliability.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "074656d9-0c22-4222-bf51-fa41dde25a60",
   "metadata": {},
   "source": [
    "### Q14 How can we address multicollinearity in regression?\n",
    "\n",
    "Multicollinearity can be addressed using the following strategies:\n",
    "\n",
    "1. **Remove Highly Correlated Variables**:\n",
    "   - Eliminate one of the correlated predictors from the model.\n",
    "\n",
    "2. **Combine Variables**:\n",
    "   - Use techniques like Principal Component Analysis (PCA) to reduce dimensionality and combine correlated features.\n",
    "\n",
    "3. **Regularization Techniques**:\n",
    "   - Apply Ridge or Lasso regression, which penalize large coefficients and reduce the impact of multicollinearity.\n",
    "\n",
    "4. **Variance Inflation Factor (VIF)**:\n",
    "   - Identify variables with high VIF values (> 5 or > 10) and either remove or combine them.\n",
    "\n",
    "5. **Domain Knowledge**:\n",
    "   - Use knowledge of the problem to decide which variables to keep or remove.\n",
    "\n",
    "By reducing multicollinearity, the model becomes more interpretable and stable.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e30c18e2-4fb9-4ff4-a896-3f9e189b83e9",
   "metadata": {},
   "source": [
    "### Q15 How can feature selection improve model performance in regression analysis?\n",
    "\n",
    "Feature selection improves regression analysis by:\n",
    "\n",
    "1. **Reducing Overfitting**:\n",
    "   - Eliminating irrelevant features reduces noise, leading to better generalization on unseen data.\n",
    "\n",
    "2. **Improving Interpretability**:\n",
    "   - A simpler model with fewer features is easier to understand and explain.\n",
    "\n",
    "3. **Enhancing Computational Efficiency**:\n",
    "   - Fewer features reduce the time and resources required for training.\n",
    "\n",
    "4. **Reducing Multicollinearity**:\n",
    "   - By selecting the most relevant predictors, multicollinearity is minimized.\n",
    "\n",
    "### Common Methods for Feature Selection:\n",
    "1. **Filter Methods**:\n",
    "   - Based on statistical tests (e.g., correlation, chi-square).\n",
    "2. **Wrapper Methods**:\n",
    "   - Use iterative approaches like forward selection, backward elimination, or recursive feature elimination (RFE).\n",
    "3. **Embedded Methods**:\n",
    "   - Built into models like Lasso (L1 regularization) to select important features automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be3ea3e-3a5f-4a53-8eae-4b65a0a279a6",
   "metadata": {},
   "source": [
    "### Q16 How is Adjusted R-squared calculated?\n",
    "\n",
    "Adjusted R-squared is a modified version of R-squared that accounts for the number of predictors in the model. It penalizes adding variables that do not improve the model’s fit.\n",
    "\n",
    "### Formula:\n",
    "\\[\n",
    "\\text{Adjusted R}^2 = 1 - \\left( \\frac{(1 - R^2)(n - 1)}{n - k - 1} \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( R^2 \\): Regular R-squared\n",
    "- \\( n \\): Number of observations\n",
    "- \\( k \\): Number of predictors\n",
    "\n",
    "### Key Features:\n",
    "- Increases only if the new predictor improves the model's fit more than expected by chance.\n",
    "- Decreases if the added predictor does not contribute significantly.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8be0644-2dc2-4cf7-9749-ba4fa15a7e54",
   "metadata": {},
   "source": [
    "### Q17 Why is MSE sensitive to outliers?\n",
    "\n",
    "Mean Squared Error (MSE) is sensitive to outliers because it squares the residuals (errors) during calculation:\n",
    "\n",
    "\\[\n",
    "\\text{MSE} = \\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2\n",
    "\\]\n",
    "\n",
    "### Effects of Squaring Errors:\n",
    "1. **Amplifies Large Errors**:\n",
    "   - Squaring the residuals disproportionately increases the impact of large errors compared to small ones.\n",
    "2. **Model Bias**:\n",
    "   - The model may become biased toward minimizing large errors, potentially distorting predictions for the majority of data points.\n",
    "\n",
    "### Addressing Sensitivity:\n",
    "- Use alternative metrics like Mean Absolute Error (MAE) or Huber loss, which are less sensitive to outliers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d2a80b-32a1-4c4f-b456-81882fecfbb7",
   "metadata": {},
   "source": [
    "### Q18 What is the role of homoscedasticity in linear regression?\n",
    "\n",
    "**Homoscedasticity** refers to the assumption that the variance of residuals (errors) is constant across all levels of the independent variables.\n",
    "\n",
    "### Why It Matters:\n",
    "1. **Reliable Standard Errors**:\n",
    "   - Homoscedasticity ensures accurate calculation of standard errors, which affects hypothesis testing and confidence intervals.\n",
    "\n",
    "2. **Unbiased Estimates**:\n",
    "   - When residuals have constant variance, the estimated coefficients are efficient and unbiased.\n",
    "\n",
    "3. **Valid Inferences**:\n",
    "   - Heteroscedasticity can lead to inflated Type I or Type II errors, making statistical inferences unreliable.\n",
    "\n",
    "### Detecting Heteroscedasticity:\n",
    "1. **Residual Plot**:\n",
    "   - Plot residuals against fitted values. A \"funnel-shaped\" pattern suggests heteroscedasticity.\n",
    "2. **Breusch-Pagan Test**:\n",
    "   - A statistical test for detecting heteroscedasticity.\n",
    "\n",
    "### Addressing Heteroscedasticity:\n",
    "- Transform the dependent variable (e.g., log transformation).\n",
    "- Use robust standard errors or weighted least squares (WLS).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4277d6a-1551-40ab-a711-5902945b7e0b",
   "metadata": {},
   "source": [
    "### Q19 What is Root Mean Squared Error (RMSE)?\n",
    "\n",
    "Root Mean Squared Error (RMSE) is a commonly used metric for evaluating the performance of regression models. It measures the average magnitude of prediction errors, giving higher weight to larger errors due to squaring.\n",
    "\n",
    "### Formula:\n",
    "\\[\n",
    "\\text{RMSE} = \\sqrt{\\frac{1}{n} \\sum_{i=1}^{n} (y_i - \\hat{y}_i)^2}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( y_i \\): Actual values\n",
    "- \\( \\hat{y}_i \\): Predicted values\n",
    "- \\( n \\): Number of observations\n",
    "\n",
    "### Characteristics:\n",
    "1. **Interpretability**:\n",
    "   - RMSE is in the same units as the dependent variable, making it easier to interpret.\n",
    "2. **Sensitivity to Outliers**:\n",
    "   - RMSE is sensitive to large errors because it squares residuals.\n",
    "\n",
    "A lower RMSE indicates a better-fitting model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05aafef7-5060-4844-82f3-99984023d34b",
   "metadata": {},
   "source": [
    "### Q20 Why is pickling considered risky?\n",
    "\n",
    "Pickling can pose security and compatibility risks when saving and loading objects in Python.\n",
    "\n",
    "### Risks:\n",
    "1. **Security Vulnerabilities**:\n",
    "   - Pickled files can execute arbitrary code during deserialization, making them vulnerable to malicious attacks if the file source is untrusted.\n",
    "\n",
    "2. **Version Dependency**:\n",
    "   - Pickled objects may not be compatible with future versions of Python or libraries, leading to deserialization errors.\n",
    "\n",
    "3. **Platform Dependency**:\n",
    "   - Pickled files may not work across different operating systems or Python environments.\n",
    "\n",
    "### Best Practices:\n",
    "- Only load pickle files from trusted sources.\n",
    "- Consider alternative formats like joblib or JSON for safer and more portable serialization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70f4388f-d2df-4e57-81d5-5beaada0f81b",
   "metadata": {},
   "source": [
    "### Q21 What alternatives exist to pickling for saving ML models?\n",
    "\n",
    "Several alternatives to pickling provide better security, compatibility, and performance for saving machine learning models:\n",
    "\n",
    "### Alternatives:\n",
    "1. **Joblib**:\n",
    "   - Specializes in saving large NumPy arrays and Scikit-learn models efficiently.\n",
    "   - Example:\n",
    "     ```python\n",
    "     import joblib\n",
    "     joblib.dump(model, 'model.joblib')\n",
    "     model = joblib.load('model.joblib')\n",
    "     ```\n",
    "\n",
    "2. **ONNX (Open Neural Network Exchange)**:\n",
    "   - A framework-agnostic format for saving models, widely used in deep learning.\n",
    "\n",
    "3. **PMML (Predictive Model Markup Language)**:\n",
    "   - XML-based format for sharing machine learning models across platforms.\n",
    "\n",
    "4. **JSON**:\n",
    "   - Suitable for saving lightweight models or configurations (e.g., model weights and hyperparameters).\n",
    "\n",
    "5. **HDF5**:\n",
    "   - Used by libraries like TensorFlow/Keras for saving large models efficiently.\n",
    "\n",
    "Choosing the right alternative depends on your use case, such as model size, framework, and compatibility needs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47fc7fe-e31a-40ac-89be-7cffb9001f7e",
   "metadata": {},
   "source": [
    "### Q22 What is heteroscedasticity, and why is it a problem?\n",
    "\n",
    "Heteroscedasticity occurs when the variance of residuals (errors) is not constant across all levels of the independent variable(s).\n",
    "\n",
    "### Causes:\n",
    "1. Skewed distributions in the data.\n",
    "2. Presence of outliers.\n",
    "3. Incorrect functional form of the model.\n",
    "\n",
    "### Why It’s a Problem:\n",
    "1. **Unreliable Standard Errors**:\n",
    "   - Heteroscedasticity can lead to incorrect confidence intervals and p-values.\n",
    "\n",
    "2. **Model Inefficiency**:\n",
    "   - The Ordinary Least Squares (OLS) estimates remain unbiased but are no longer efficient.\n",
    "\n",
    "3. **Invalid Hypothesis Testing**:\n",
    "   - Inaccurate test statistics may lead to incorrect conclusions.\n",
    "\n",
    "### Solutions:\n",
    "1. **Transformation**:\n",
    "   - Apply log or square root transformations to stabilize variance.\n",
    "2. **Weighted Least Squares (WLS)**:\n",
    "   - Assign weights to observations based on their variance.\n",
    "3. **Robust Standard Errors**:\n",
    "   - Use robust methods to adjust for heteroscedasticity.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154eda4f-bd83-45b8-a2d8-c183177c5758",
   "metadata": {},
   "source": [
    "### Q23 How can interaction terms enhance a regression model's predictive power?\n",
    "\n",
    "Interaction terms account for the combined effect of two or more variables on the dependent variable, capturing relationships that are not explained by individual variables alone.\n",
    "\n",
    "### Example:\n",
    "Suppose we are modeling the relationship:\n",
    "\\[\n",
    "y = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + \\beta_3 (x_1 \\cdot x_2) + \\epsilon\n",
    "\\]\n",
    "Here, \\( x_1 \\cdot x_2 \\) is the interaction term.\n",
    "\n",
    "### Benefits:\n",
    "1. **Captures Synergistic Effects**:\n",
    "   - Interaction terms reveal how the effect of one variable depends on the level of another.\n",
    "\n",
    "2. **Improves Model Fit**:\n",
    "   - Models with interaction terms often provide a better fit to the data.\n",
    "\n",
    "3. **Enhances Interpretability**:\n",
    "   - Identifies complex relationships that are otherwise missed by linear terms alone.\n",
    "\n",
    "### Practical Use:\n",
    "- Useful in scenarios like marketing (e.g., the combined effect of price and advertising) or biology (e.g., interaction between two genes).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25376708-e900-491a-ba06-0bb9b9babba2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "95991990-1475-4ce4-a3f9-ebc846e6b6be",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58bfa4f5-5964-460e-8469-ba94173a02bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4db2b092-eb51-4ada-b7a9-2e4e5d0b8ac8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3c4dc16-64ed-4c97-a605-0ef6c78fa3a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0beb561f-7e62-4119-9a39-d0090fae1e4c",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b92a8365-9f0a-4800-acf1-ed1accb0468d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aa1b80a2-2e13-43e3-8f73-b44537973c21",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f598154c-92c7-4bdb-8812-29f13a34d1d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d857684b-ff3a-477d-94e1-99a922fa89b3",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3efa3cd5-0fe7-4edf-8980-eef5686ebe95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35b6f0b0-2cc3-4130-90f1-154394c812d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

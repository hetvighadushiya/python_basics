{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "de5af9b0-18e0-45dd-8d62-82fa0f31de7d",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca941ed6-e14e-426b-a8d7-72c66c6e183e",
   "metadata": {},
   "source": [
    "# Logistic Regression Theory\n",
    "\n",
    "### 1) What is Logistic Regression, and how does it differ from Linear Regression?\n",
    "Logistic Regression is a statistical model used for binary classification tasks, where the goal is to predict one of two possible outcomes. It models the probability that a given input point belongs to a particular class.\n",
    "\n",
    "**Difference from Linear Regression:**\n",
    "- **Linear Regression** is used for predicting continuous outcomes, and it predicts a linear relationship between the independent variables and the dependent variable.\n",
    "- **Logistic Regression** is used for classification and predicts the probability of a binary outcome using a logistic (sigmoid) function. It transforms the output of a linear model into a value between 0 and 1.\n",
    "\n",
    "### 2) What is the mathematical equation of Logistic Regression?\n",
    "The mathematical equation of Logistic Regression is:\n",
    "\n",
    "\\[\n",
    "p(y=1|X) = \\frac{1}{1 + e^{-z}}\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( p(y=1|X) \\) is the probability of the outcome being 1 given the input features \\( X \\).\n",
    "- \\( z = \\beta_0 + \\beta_1 x_1 + \\beta_2 x_2 + ... + \\beta_n x_n \\) is the linear combination of input features and their corresponding coefficients.\n",
    "- \\( e \\) is Euler’s number (base of the natural logarithm).\n",
    "\n",
    "### 3) Why do we use the Sigmoid function in Logistic Regression?\n",
    "The Sigmoid function is used to map any real-valued number into a value between 0 and 1. This is important because logistic regression is used for binary classification, and we need the output to represent the probability of the target variable belonging to one of the two classes. The Sigmoid function ensures that the output is constrained to the range [0, 1], making it interpretable as a probability.\n",
    "\n",
    "### 4) What is the cost function of Logistic Regression?\n",
    "The cost function in Logistic Regression is the **Log Loss** or **Binary Cross-Entropy Loss**:\n",
    "\n",
    "\\[\n",
    "J(\\theta) = - \\frac{1}{m} \\sum_{i=1}^{m} \\left( y^{(i)} \\log(h_{\\theta}(x^{(i)})) + (1 - y^{(i)}) \\log(1 - h_{\\theta}(x^{(i)})) \\right)\n",
    "\\]\n",
    "\n",
    "Where:\n",
    "- \\( m \\) is the number of training examples.\n",
    "- \\( h_{\\theta}(x) \\) is the predicted probability of the positive class (calculated using the Sigmoid function).\n",
    "- \\( y^{(i)} \\) is the true label of the i-th training example.\n",
    "\n",
    "### 5) What is Regularization in Logistic Regression? Why is it needed?\n",
    "Regularization is a technique used to prevent overfitting by adding a penalty term to the cost function. In Logistic Regression, regularization discourages the model from fitting excessively complex models that perform well on training data but poorly on new data.\n",
    "\n",
    "### 6) Explain the difference between Lasso, Ridge, and Elastic Net regression.\n",
    "- **Lasso (L1 regularization)**: Adds a penalty equal to the absolute value of the coefficients. It can shrink some coefficients to zero, effectively performing feature selection.\n",
    "  \n",
    "- **Ridge (L2 regularization)**: Adds a penalty equal to the squared value of the coefficients. It encourages smaller coefficients but does not eliminate them completely.\n",
    "  \n",
    "- **Elastic Net**: A combination of L1 and L2 regularization. It balances the properties of Lasso and Ridge, making it useful when there are multiple correlated features.\n",
    "\n",
    "### 7) When should we use Elastic Net instead of Lasso or Ridge?\n",
    "Elastic Net is preferred when:\n",
    "- There are many correlated features.\n",
    "- You want to combine the feature selection property of Lasso with the regularization ability of Ridge.\n",
    "\n",
    "If you suspect that some features are highly correlated, Elastic Net is often a better choice than using Lasso or Ridge alone.\n",
    "\n",
    "### 8) What is the impact of the regularization parameter (λ) in Logistic Regression?\n",
    "The regularization parameter \\( \\lambda \\) controls the strength of the penalty applied to the coefficients. \n",
    "- A larger \\( \\lambda \\) value increases the penalty, leading to more shrinkage of the coefficients, potentially reducing overfitting but also making the model underfit.\n",
    "- A smaller \\( \\lambda \\) value reduces the penalty, allowing the model to fit more closely to the training data, but it may overfit if \\( \\lambda \\) is too small.\n",
    "\n",
    "### 9) What are the key assumptions of Logistic Regression?\n",
    "The key assumptions of Logistic Regression are:\n",
    "- The dependent variable is binary.\n",
    "- The relationship between the independent variables and the log-odds of the dependent variable is linear.\n",
    "- Observations are independent of each other.\n",
    "- No or little multicollinearity exists between the independent variables.\n",
    "\n",
    "### 10) What are some alternatives to Logistic Regression for classification tasks?\n",
    "Alternatives to Logistic Regression include:\n",
    "- **Decision Trees**: Non-linear models that split the data based on feature thresholds.\n",
    "- **Random Forests**: An ensemble of decision trees that aggregates results for better accuracy.\n",
    "- **Support Vector Machines (SVM)**: A model that finds the optimal hyperplane to separate classes.\n",
    "- **K-Nearest Neighbors (KNN)**: A non-parametric algorithm that classifies based on the closest neighbors in the feature space.\n",
    "- **Neural Networks**: A class of models inspired by biological neural networks, capable of modeling complex non-linear relationships.\n",
    "\n",
    "### 11) What are Classification Evaluation Metrics?\n",
    "Classification evaluation metrics are used to assess the performance of classification models. Common evaluation metrics include:\n",
    "\n",
    "- **Accuracy**: The proportion of correctly classified instances among the total instances.\n",
    "- **Precision**: The proportion of true positives among all predicted positives. Precision = TP / (TP + FP).\n",
    "- **Recall (Sensitivity)**: The proportion of true positives among all actual positives. Recall = TP / (TP + FN).\n",
    "- **F1-Score**: The harmonic mean of precision and recall. F1-Score = 2 * (Precision * Recall) / (Precision + Recall).\n",
    "- **Confusion Matrix**: A matrix showing the counts of true positives, false positives, true negatives, and false negatives.\n",
    "- **ROC Curve and AUC**: The Receiver Operating Characteristic curve shows the trade-off between true positive rate and false positive rate, and AUC (Area Under the Curve) provides a single metric of the model's ability to distinguish between classes.\n",
    "\n",
    "### 12) How does class imbalance affect Logistic Regression?\n",
    "Class imbalance occurs when one class is significantly more frequent than the other. It can negatively affect Logistic Regression by:\n",
    "- Biasing the model towards the majority class, which leads to poor performance on the minority class.\n",
    "- Reducing the model's ability to generalize to the minority class.\n",
    "  \n",
    "To address class imbalance, techniques such as **resampling (over-sampling or under-sampling)**, **class weights adjustment**, or using **evaluation metrics like Precision-Recall AUC** are often applied.\n",
    "\n",
    "### 13) What is Hyperparameter Tuning in Logistic Regression?\n",
    "Hyperparameter tuning involves selecting the best values for parameters that are not learned by the model during training, such as:\n",
    "- **Regularization parameter (λ)**: Controls the strength of regularization.\n",
    "- **Solver**: Determines the optimization method used to minimize the cost function.\n",
    "- **Maximum number of iterations**: Defines the stopping criterion for the optimization process.\n",
    "  \n",
    "Hyperparameter tuning is often done using techniques like **Grid Search** or **Randomized Search** to improve the model's performance.\n",
    "\n",
    "### 14) What are different solvers in Logistic Regression? Which one should be used?\n",
    "In Logistic Regression, solvers are optimization algorithms used to minimize the cost function. Common solvers include:\n",
    "- **'liblinear'**: A good choice for small datasets, supports L1 and L2 regularization.\n",
    "- **'newton-cg'**: Efficient for large datasets and works well with L2 regularization.\n",
    "- **'lbfgs'**: A robust solver suitable for larger datasets, supports L2 regularization.\n",
    "- **'saga'**: Suitable for very large datasets, supports both L1 and L2 regularization.\n",
    "\n",
    "**Recommendation**: For smaller datasets, 'liblinear' works well, while for larger datasets, 'lbfgs' or 'saga' is more efficient.\n",
    "\n",
    "### 15) How is Logistic Regression extended for multiclass classification?\n",
    "Logistic Regression can be extended for multiclass classification in two main ways:\n",
    "- **One-vs-Rest (OvR)**: For each class, a separate binary logistic regression model is trained to distinguish that class from all others.\n",
    "- **Softmax Regression (Multinomial Logistic Regression)**: Uses a generalization of the logistic function (the softmax function) to handle multiple classes simultaneously. It computes the probability of each class and assigns the class with the highest probability.\n",
    "\n",
    "### 16) What are the advantages and disadvantages of Logistic Regression?\n",
    "**Advantages**:\n",
    "- Simple and interpretable model.\n",
    "- Efficient for linearly separable data.\n",
    "- Can be regularized to prevent overfitting.\n",
    "- Fast and computationally inexpensive.\n",
    "\n",
    "**Disadvantages**:\n",
    "- Assumes linearity between the features and the log-odds.\n",
    "- Not suitable for complex relationships (non-linear).\n",
    "- Sensitive to outliers.\n",
    "- Struggles with class imbalance unless properly handled.\n",
    "\n",
    "### 17) What are some use cases of Logistic Regression?\n",
    "Logistic Regression is widely used in:\n",
    "- **Binary classification**: Spam detection, medical diagnosis (e.g., predicting if a patient has a disease), credit card fraud detection.\n",
    "- **Multiclass classification**: Handwriting recognition, image classification, sentiment analysis.\n",
    "\n",
    "### 18) What is the difference between Softmax Regression and Logistic Regression?\n",
    "- **Logistic Regression** is used for binary classification, where it models the probability of the positive class using the sigmoid function.\n",
    "- **Softmax Regression** (Multinomial Logistic Regression) is an extension of Logistic Regression used for multiclass classification, where the softmax function is used to calculate the probability for each class.\n",
    "\n",
    "### 19) How do we choose between One-vs-Rest (OvR) and Softmax for multiclass classification?\n",
    "- **One-vs-Rest (OvR)** is typically used when classes are not mutually exclusive or when the dataset is too large, and training multiple binary classifiers is computationally feasible.\n",
    "- **Softmax** is preferred when classes are mutually exclusive, and you want to calculate the probabilities for each class simultaneously, offering a more unified approach for multiclass classification.\n",
    "\n",
    "### 20) How do we interpret coefficients in Logistic Regression?\n",
    "The coefficients in Logistic Regression represent the log-odds change of the dependent variable for a one-unit change in the corresponding independent variable. Specifically:\n",
    "- If \\( \\beta_i > 0 \\), an increase in \\( x_i \\) will increase the probability of the positive class.\n",
    "- If \\( \\beta_i < 0 \\), an increase in \\( x_i \\) will decrease the probability of the positive class.\n",
    "  \n",
    "To interpret the impact of the coefficients in terms of probabilities, you can exponentiate the coefficients, which gives the **odds ratio**. For example:\n",
    "- If \\( \\beta_i = 0.5 \\), the odds ratio is \\( e^{0.5} \\approx 1.65 \\), meaning a one-unit increase in \\( x_i \\) increases the odds of the positive class by 65%.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f93861b-0a50-4f21-a72e-35bcfb37a898",
   "metadata": {},
   "source": [
    "# Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f44eada-4ad9-4d96-9bdf-989bdc846c73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Load a dataset, split it, apply Logistic Regression, and print accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "log_reg = LogisticRegression(max_iter=200)\n",
    "log_reg.fit(X_train, y_train)\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Logistic Regression Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a89249ee-5d49-412b-9e4f-45a437d2bb3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with L1 Regularization: 1.00\n"
     ]
    }
   ],
   "source": [
    "### 2) Write a Python program to apply L1 regularization (Lasso) on a dataset using LogisticRegression(penalty='l1') and print the model accuracy\n",
    "\n",
    "from sklearn.svm import l1_min_c\n",
    "\n",
    "model_l1 = LogisticRegression(penalty='l1', solver='liblinear', max_iter=200)\n",
    "model_l1.fit(X_train, y_train)\n",
    "\n",
    "y_pred_l1 = model_l1.predict(X_test)\n",
    "accuracy_l1 = accuracy_score(y_test, y_pred_l1)\n",
    "print(f'Model Accuracy with L1 Regularization: {accuracy_l1:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6699146c-efee-42e8-91fe-dc6d267905b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with L2 Regularization: 1.00\n",
      "Coefficients: [[-0.39348375  0.96248072 -2.37513667 -0.99874733]\n",
      " [ 0.50844947 -0.25480597 -0.21300937 -0.77574588]\n",
      " [-0.11496571 -0.70767474  2.58814604  1.77449321]]\n"
     ]
    }
   ],
   "source": [
    "### 3) Write a Python program to train Logistic Regression with L2 regularization (Ridge) using LogisticRegression(penalty='l2'). Print model accuracy and coefficients\n",
    "\n",
    "model_l2 = LogisticRegression(penalty='l2', max_iter=200)\n",
    "model_l2.fit(X_train, y_train)\n",
    "\n",
    "y_pred_l2 = model_l2.predict(X_test)\n",
    "accuracy_l2 = accuracy_score(y_test, y_pred_l2)\n",
    "print(f'Model Accuracy with L2 Regularization: {accuracy_l2:.2f}')\n",
    "print('Coefficients:', model_l2.coef_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5eb9a2bf-da9f-4f27-b17c-f0a187812a14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with Elastic Net Regularization: 1.00\n"
     ]
    }
   ],
   "source": [
    "### 4) Write a Python program to train Logistic Regression with Elastic Net Regularization (penalty='elasticnet')\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_elastic = LogisticRegression(penalty='elasticnet', solver='saga', l1_ratio=0.5, max_iter=1000)\n",
    "model_elastic.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_elastic = model_elastic.predict(X_test_scaled)\n",
    "accuracy_elastic = accuracy_score(y_test, y_pred_elastic)\n",
    "print(f'Model Accuracy with Elastic Net Regularization: {accuracy_elastic:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d67f066c-c43f-4976-9f65-1f18524d2f05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy with OVR (One-vs-Rest): 0.97\n"
     ]
    }
   ],
   "source": [
    "### 5) Write a Python program to train a Logistic Regression model for multiclass classification using multi_class='ovr'\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model_ovr = OneVsRestClassifier(LogisticRegression(max_iter=200))\n",
    "model_ovr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ovr = model_ovr.predict(X_test)\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "\n",
    "print(f'Model Accuracy with OVR (One-vs-Rest): {accuracy_ovr:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cba5477b-d77c-4d4e-9ccb-627252d6be43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Parameters: {'C': 10, 'penalty': 'l1'}\n",
      "Best Accuracy: 0.96\n"
     ]
    }
   ],
   "source": [
    "### 6) Write a Python program to apply GridSearchCV to tune the hyperparameters (C and penalty) of Logistic Regression. Print the best parameters and accuracy\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.01, 0.1, 1, 10],\n",
    "    'penalty': ['l1', 'l2']\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear'), param_grid, cv=5, scoring='accuracy')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f'Best Parameters: {grid_search.best_params_}')\n",
    "print(f'Best Accuracy: {grid_search.best_score_:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "399ec729-6560-44d6-b70b-6e76c3697462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Accuracy: 0.97\n"
     ]
    }
   ],
   "source": [
    "### 7) Write a Python program to evaluate Logistic Regression using Stratified K-Fold Cross-Validation. Print the average accuracy\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "cv_scores = cross_val_score(LogisticRegression(max_iter=200), X, y, cv=skf, scoring='accuracy')\n",
    "\n",
    "print(f'Average Accuracy: {np.mean(cv_scores):.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "51ecdcbc-95eb-4a85-b0e1-5a15f1e56969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy from CSV Data: 1.00\n"
     ]
    }
   ],
   "source": [
    "### 8) Write a Python program to load a dataset from a CSV file, apply Logistic Regression, and evaluate its accuracy\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "csv_path = \"C:/Users/hetvi/Downloads/superstore.csv\"\n",
    "df = pd.read_csv(csv_path, encoding='latin1') \n",
    "\n",
    "df = pd.get_dummies(df)\n",
    "\n",
    "df = df.dropna()\n",
    "\n",
    "y = df.iloc[:, -1]  \n",
    "X = df.iloc[:, :-1] \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_csv = LogisticRegression(max_iter=200)\n",
    "model_csv.fit(X_train, y_train)\n",
    "\n",
    "y_pred_csv = model_csv.predict(X_test)\n",
    "accuracy_csv = accuracy_score(y_test, y_pred_csv)\n",
    "print(f'Model Accuracy from CSV Data: {accuracy_csv:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c6da89-3eb3-4284-9a30-4a58a6f362e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'C': 8.424426408004217, 'penalty': 'l2', 'solver': 'saga'}\n",
      "Best Cross-Validation Score: 0.9667\n"
     ]
    }
   ],
   "source": [
    "### q9)\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV, train_test_split\n",
    "from scipy.stats import uniform\n",
    "\n",
    "X, y = load_iris(return_X_y=True)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_distributions = {\n",
    "    'C': uniform(0.1, 10),         \n",
    "    'penalty': ['l1', 'l2'],       \n",
    "    'solver': ['liblinear', 'saga']  \n",
    "}\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=20,          \n",
    "    cv=5,                \n",
    "    random_state=42,\n",
    "    n_jobs=-1           \n",
    ")\n",
    "\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Hyperparameters:\", random_search.best_params_)\n",
    "print(\"Best Cross-Validation Score: {:.4f}\".format(random_search.best_score_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "911dc003-b42e-4958-8735-f2c297f6410f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "### 10) Implement One-vs-One Multiclass Logistic Regression\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "model = OneVsOneClassifier(LogisticRegression(solver='liblinear'))\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Accuracy:\", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24fa4bc-4add-4c0a-b1d8-0a42f4bf4475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### 11) Train Logistic Regression and visualize Confusion Matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"Actual\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fdda1bd8-0cac-48f3-aa37-00ea82970b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8666666666666667\n",
      "Recall: 0.8198198198198198\n",
      "F1 Score: 0.8425925925925926\n"
     ]
    }
   ],
   "source": [
    "### 12) Evaluate performance using Precision, Recall, and F1-Score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# Compute metrics\n",
    "precision = precision_score(y_test, y_pred)\n",
    "recall = recall_score(y_test, y_pred)\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 Score:\", f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d1fcb7-51f9-48bc-96f1-9f7078ad0813",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.865\n"
     ]
    }
   ],
   "source": [
    "### 13) Train Logistic Regression on imbalanced data with class weights\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "X, y = make_classification(n_samples=1000, weights=[0.9, 0.1], random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0ce43bc7-c229-4cba-99f3-e9fe89d59770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7552447552447552\n"
     ]
    }
   ],
   "source": [
    "### 14) Train Logistic Regression on Titanic dataset\n",
    "import seaborn as sns\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "df = sns.load_dataset(\"titanic\")\n",
    "\n",
    "df = df[['age', 'fare', 'pclass', 'sex', 'survived']].dropna()\n",
    "df['sex'] = df['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "X = df[['age', 'fare', 'pclass', 'sex']]\n",
    "y = df['survived']\n",
    "\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X = imputer.fit_transform(X)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy:\", accuracy_score(y_test, model.predict(X_test)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8482b2c2-813e-4a10-a240-4e89b22b771b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with scaling: 0.7412587412587412\n"
     ]
    }
   ],
   "source": [
    "### 15) Apply Standardization before training Logistic Regression\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LogisticRegression()\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(\"Accuracy with scaling:\", accuracy_score(y_test, model_scaled.predict(X_test_scaled)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea2219ad-a56d-4cab-9e15-12a2a2c7d43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 16) Evaluate performance using ROC-AUC score\n",
    "accuracy_original = accuracy_score(y_test, model.predict(X_test))\n",
    "accuracy_scaled = accuracy_score(y_test, model_scaled.predict(X_test_scaled))\n",
    "\n",
    "print(\"Accuracy without scaling:\", accuracy_original)\n",
    "print(\"Accuracy with scaling:\", accuracy_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3dd16a9a-f682-4684-8a62-17c92e87235d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC-AUC Score: 0.8109605911330049\n"
     ]
    }
   ],
   "source": [
    "### 17) Train Logistic Regression with custom learning rate (C=0.5)\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "# Compute ROC-AUC Score\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "roc_auc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"ROC-AUC Score:\", roc_auc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54e1679e-34f6-414e-8c5b-3fb7dbf0a538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy with C=0.5: 0.7482517482517482\n"
     ]
    }
   ],
   "source": [
    "### 18) Identify important features based on coefficients\n",
    "model_custom = LogisticRegression(C=0.5)\n",
    "model_custom.fit(X_train, y_train)\n",
    "\n",
    "print(\"Accuracy with C=0.5:\", accuracy_score(y_test, model_custom.predict(X_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "315fb40e-2d32-454d-ade0-e5b01bb1dc2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature  Coefficient\n",
      "3     sex     2.530478\n",
      "1    fare     0.000227\n",
      "0     age    -0.042472\n",
      "2  pclass    -1.240688\n"
     ]
    }
   ],
   "source": [
    "### 19) Evaluate performance using Cohen’s Kappa Score\n",
    "# Feature importance\n",
    "feature_importance = pd.DataFrame({'Feature': ['age', 'fare', 'pclass', 'sex'], \n",
    "                                   'Coefficient': model.coef_[0]})\n",
    "print(feature_importance.sort_values(by='Coefficient', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "bcab6cd5-e3d6-4aac-ab58-832042c8a3cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cohen's Kappa Score: 0.6581196581196581\n"
     ]
    }
   ],
   "source": [
    "### 20) Visualize the Precision-Recall Curve\n",
    "from sklearn.metrics import cohen_kappa_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import make_classification\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "kappa_score = cohen_kappa_score(y_test, y_pred)\n",
    "print(\"Cohen's Kappa Score:\", kappa_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "10688098-5112-415c-bf94-624beb71e91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Solver: liblinear, Accuracy: 1.0000\n",
      "Solver: saga, Accuracy: 1.0000\n",
      "Solver: lbfgs, Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "### 21) Train Logistic Regression with Different Solvers and Compare Accuracy\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "solvers = ['liblinear', 'saga', 'lbfgs']\n",
    "\n",
    "for solver in solvers:\n",
    "    if solver == 'liblinear':\n",
    "        model = OneVsRestClassifier(LogisticRegression(solver=solver, max_iter=5000))\n",
    "    else:\n",
    "        model = LogisticRegression(solver=solver, max_iter=5000)  # Default to multinomial for multi-class\n",
    "\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Solver: {solver}, Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7ef832-4d09-4e48-8c86-5ca90fd5db77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matthews Correlation Coefficient (MCC): 0.6593\n"
     ]
    }
   ],
   "source": [
    "### 22) Train Logistic Regression and Evaluate using Matthews Correlation Coefficient (MCC)\n",
    "\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "\n",
    "X, y = make_classification(n_samples=1000, n_features=10, random_state=42)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(solver='liblinear')\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "print(f\"Matthews Correlation Coefficient (MCC): {mcc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c01fab8-bc7f-4e56-8459-93595f68f78c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on Raw Data: 1.0000\n",
      "Accuracy on Standardized Data: 1.0000\n"
     ]
    }
   ],
   "source": [
    "### 23) Train Logistic Regression on Raw and Standardized Data\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Load dataset\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model_raw = LogisticRegression(max_iter=5000)\n",
    "model_raw.fit(X_train, y_train)\n",
    "y_pred_raw = model_raw.predict(X_test)\n",
    "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "model_scaled = LogisticRegression(max_iter=5000)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "y_pred_scaled = model_scaled.predict(X_test_scaled)\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "\n",
    "print(f\"Accuracy on Raw Data: {accuracy_raw:.4f}\")\n",
    "print(f\"Accuracy on Standardized Data: {accuracy_scaled:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b85648fe-7e77-4019-8468-8e293e5f8de3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best C value: 10\n",
      "Best Accuracy: 0.9583\n"
     ]
    }
   ],
   "source": [
    "### 24) Find Optimal C (Regularization Strength) using Cross-Validation\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "param_grid = {'C': [0.01, 0.1, 1, 10, 100]}\n",
    "\n",
    "grid_search = GridSearchCV(LogisticRegression(solver='liblinear', max_iter=5000), param_grid, cv=5)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best C value: {grid_search.best_params_['C']}\")\n",
    "print(f\"Best Accuracy: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d3dcd279-9883-4bd6-8806-fc1da22f34bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted classes: [1 0 2 1 1 0 1 2 1 1 2 0 0 0 0 1 2 1 1 2 0 2 0 2 2 2 2 2 0 0]\n"
     ]
    }
   ],
   "source": [
    "### 25) Train Logistic Regression, Save Model using Joblib, and Load to Make Predictions\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "model = LogisticRegression(max_iter=5000)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "joblib.dump(model, 'logistic_regression_model.pkl')\n",
    "\n",
    "loaded_model = joblib.load('logistic_regression_model.pkl')\n",
    "\n",
    "y_pred = loaded_model.predict(X_test)\n",
    "print(f\"Predicted classes: {y_pred}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd2c9ee-529a-44a2-b7bf-e011b12b138a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

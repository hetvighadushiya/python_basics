{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24088541-9a11-4ce4-9906-90170ca4b08e",
   "metadata": {},
   "source": [
    "### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf4462db-f317-4d92-b536-fa0340565b63",
   "metadata": {},
   "source": [
    "# Theory Questions\n",
    "\n",
    "## 1. What is a Decision Tree, and how does it work?\n",
    "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. It works by recursively splitting the dataset based on feature values, forming a tree-like structure where each internal node represents a decision rule, branches represent possible outcomes, and leaf nodes represent final predictions.\n",
    "\n",
    "## 2. What are impurity measures in Decision Trees?\n",
    "Impurity measures are metrics used to evaluate how mixed a node is in terms of class labels. Common impurity measures include:\n",
    "- Gini Impurity\n",
    "- Entropy\n",
    "\n",
    "## 3. What is the mathematical formula for Gini Impurity?\n",
    "The Gini Impurity formula is given by:\n",
    "\n",
    "\\[\n",
    "Gini = 1 - \\sum_{i=1}^{C} p_i^2\n",
    "\\]\n",
    "\n",
    "where \\( p_i \\) is the probability of class \\( i \\) in the dataset.\n",
    "\n",
    "## 4. What is the mathematical formula for Entropy?\n",
    "Entropy is calculated as:\n",
    "\n",
    "\\[\n",
    "Entropy = - \\sum_{i=1}^{C} p_i \\log_2 p_i\n",
    "\\]\n",
    "\n",
    "where \\( p_i \\) is the probability of class \\( i \\) in the dataset.\n",
    "\n",
    "## 5. What is Information Gain, and how is it used in Decision Trees?\n",
    "Information Gain measures the reduction in entropy or impurity after a dataset split. It is used in Decision Trees to determine the best feature to split on. It is calculated as:\n",
    "\n",
    "\\[\n",
    "IG = Entropy(parent) - \\sum_{i} \\frac{|D_i|}{|D|} Entropy(D_i)\n",
    "\\]\n",
    "\n",
    "where \\( D_i \\) are the subsets created by the split.\n",
    "\n",
    "## 6. What is the difference between Gini Impurity and Entropy?\n",
    "- Gini Impurity measures the probability of incorrect classification.\n",
    "- Entropy measures the level of disorder or uncertainty in the dataset.\n",
    "- Entropy involves logarithmic calculations, making it computationally more expensive than Gini.\n",
    "\n",
    "## 7. What is the mathematical explanation behind Decision Trees?\n",
    "Decision Trees use a recursive partitioning method where:\n",
    "1. The best feature for splitting is selected using Information Gain or Gini Impurity.\n",
    "2. The dataset is split into subsets based on the chosen feature.\n",
    "3. The process is repeated until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
    "\n",
    "## 8. What is Pre-Pruning in Decision Trees?\n",
    "Pre-Pruning is a technique where the tree's growth is stopped early based on predefined criteria, such as maximum depth or minimum samples required for a split, to prevent overfitting.\n",
    "\n",
    "## 9. What is Post-Pruning in Decision Trees?\n",
    "Post-Pruning involves growing the full tree first and then trimming branches that do not improve performance, based on validation data.\n",
    "\n",
    "## 10. What is the difference between Pre-Pruning and Post-Pruning?\n",
    "- Pre-Pruning stops the tree early to prevent unnecessary growth.\n",
    "- Post-Pruning removes branches from a fully grown tree to reduce overfitting.\n",
    "\n",
    "## 11. What is a Decision Tree Regressor?\n",
    "A Decision Tree Regressor is a type of Decision Tree used for regression tasks, where the target variable is continuous rather than categorical.\n",
    "\n",
    "## 12. What are the advantages and disadvantages of Decision Trees?\n",
    "### Advantages:\n",
    "- Easy to interpret and visualize.\n",
    "- Requires little data preprocessing.\n",
    "- Handles both numerical and categorical data.\n",
    "\n",
    "### Disadvantages:\n",
    "- Prone to overfitting.\n",
    "- Sensitive to noisy data.\n",
    "- Can be unstable with small changes in data.\n",
    "\n",
    "## 13. How does a Decision Tree handle missing values?\n",
    "Decision Trees can handle missing values by:\n",
    "- Using surrogate splits (alternative splits for missing values).\n",
    "- Assigning the most common value of the feature.\n",
    "- Ignoring missing values during splits.\n",
    "\n",
    "## 14. How does a Decision Tree handle categorical features?\n",
    "Categorical features are handled by:\n",
    "- One-hot encoding.\n",
    "- Splitting based on unique category values.\n",
    "- Using label encoding with tree algorithms that support categorical splits.\n",
    "\n",
    "## 15. What are some real-world applications of Decision Trees?\n",
    "- **Medical Diagnosis**: Identifying diseases based on symptoms.\n",
    "- **Credit Scoring**: Assessing loan eligibility.\n",
    "- **Fraud Detection**: Identifying fraudulent transactions.\n",
    "- **Customer Segmentation**: Grouping customers based on behavior.\n",
    "- **Recommendation Systems**: Suggesting products based on user preferences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2debd8a5-19bf-4217-bfc1-ee64e3b8b690",
   "metadata": {},
   "source": [
    "### 16. Train a Decision Tree Classifier on the Iris dataset and print accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb126407-0c3f-4d06-8db3-e79af95f77ec",
   "metadata": {},
   "source": [
    "### Practical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d061c4e0-d36a-4bcd-9fe7-cbbe0c08c3ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0910b74-dc44-4e05-8cec-dbb0b6353a2e",
   "metadata": {},
   "source": [
    "### 17. Train a Decision Tree Classifier using Gini Impurity and print feature importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6edff17f-132f-4778-89fc-a21009efee62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances: [0.01667014 0.01667014 0.88947325 0.07718647]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "clf_gini = DecisionTreeClassifier(criterion=\"gini\")\n",
    "clf_gini.fit(X_train, y_train) \n",
    "\n",
    "print(\"Feature Importances:\", clf_gini.feature_importances_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab9e6ac-306f-4eed-97d2-f39480262ea1",
   "metadata": {},
   "source": [
    "### 18. Train a Decision Tree Classifier using Entropy and print accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a11d63ae-7bdb-4709-924a-0ca421f4a59a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy (Entropy): 1.0000\n"
     ]
    }
   ],
   "source": [
    "clf_entropy = DecisionTreeClassifier(criterion=\"entropy\")\n",
    "clf_entropy.fit(X_train, y_train)\n",
    "\n",
    "y_pred_entropy = clf_entropy.predict(X_test)\n",
    "\n",
    "accuracy_entropy = accuracy_score(y_test, y_pred_entropy)\n",
    "print(f\"Model Accuracy (Entropy): {accuracy_entropy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afb9a7-39eb-4729-be65-7b0a0ba0687d",
   "metadata": {},
   "source": [
    "### 19. Train a Decision Tree Regressor on a housing dataset and evaluate using MSE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "548ca430-5c13-4213-afdc-3328a99c51e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 0.5033\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "X_housing, y_housing = housing.data, housing.target\n",
    "\n",
    "X_train_h, X_test_h, y_train_h, y_test_h = train_test_split(X_housing, y_housing, test_size=0.2, random_state=42)\n",
    "\n",
    "regressor = DecisionTreeRegressor()\n",
    "regressor.fit(X_train_h, y_train_h)\n",
    "\n",
    "y_pred_housing = regressor.predict(X_test_h)\n",
    "\n",
    "mse = mean_squared_error(y_test_h, y_pred_housing)\n",
    "print(f\"Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04d5180-00d3-4c5d-a513-de6529305f19",
   "metadata": {},
   "source": [
    "### 20. Train a Decision Tree Classifier and visualize it using Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9c8e584f-fa41-46e9-9c1b-6e7b5c0ee2cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: graphviz in c:\\users\\hetvi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (0.20.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "!pip install graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7bbcc4df-4efd-4179-a954-6693b54d5f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<?xml version=\"1.0\" encoding=\"UTF-8\" standalone=\"no\"?>\n",
       "<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n",
       " \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n",
       "<!-- Generated by graphviz version 12.2.1 (20241206.2353)\n",
       " -->\n",
       "<!-- Title: Tree Pages: 1 -->\n",
       "<svg width=\"545pt\" height=\"447pt\"\n",
       " viewBox=\"0.00 0.00 545.38 447.25\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g id=\"graph0\" class=\"graph\" transform=\"scale(1 1) rotate(0) translate(4 443.25)\">\n",
       "<title>Tree</title>\n",
       "<polygon fill=\"white\" stroke=\"none\" points=\"-4,4 -4,-443.25 541.38,-443.25 541.38,4 -4,4\"/>\n",
       "<!-- 0 -->\n",
       "<g id=\"node1\" class=\"node\">\n",
       "<title>0</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M261.38,-439.25C261.38,-439.25 126.88,-439.25 126.88,-439.25 120.88,-439.25 114.88,-433.25 114.88,-427.25 114.88,-427.25 114.88,-364.5 114.88,-364.5 114.88,-358.5 120.88,-352.5 126.88,-352.5 126.88,-352.5 261.38,-352.5 261.38,-352.5 267.38,-352.5 273.38,-358.5 273.38,-364.5 273.38,-364.5 273.38,-427.25 273.38,-427.25 273.38,-433.25 267.38,-439.25 261.38,-439.25\"/>\n",
       "<text text-anchor=\"start\" x=\"122.88\" y=\"-421.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 2.45</text>\n",
       "<text text-anchor=\"start\" x=\"158.88\" y=\"-406.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.667</text>\n",
       "<text text-anchor=\"start\" x=\"149.5\" y=\"-390.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 150</text>\n",
       "<text text-anchor=\"start\" x=\"136.38\" y=\"-374.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"151\" y=\"-358.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 1 -->\n",
       "<g id=\"node2\" class=\"node\">\n",
       "<title>1</title>\n",
       "<path fill=\"#e58139\" stroke=\"black\" d=\"M164.38,-308.62C164.38,-308.62 71.88,-308.62 71.88,-308.62 65.88,-308.62 59.88,-302.62 59.88,-296.62 59.88,-296.62 59.88,-249.62 59.88,-249.62 59.88,-243.62 65.88,-237.62 71.88,-237.62 71.88,-237.62 164.38,-237.62 164.38,-237.62 170.38,-237.62 176.38,-243.62 176.38,-249.62 176.38,-249.62 176.38,-296.62 176.38,-296.62 176.38,-302.62 170.38,-308.62 164.38,-308.62\"/>\n",
       "<text text-anchor=\"start\" x=\"90.38\" y=\"-291.32\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"77.25\" y=\"-275.57\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 50</text>\n",
       "<text text-anchor=\"start\" x=\"67.88\" y=\"-259.82\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [50, 0, 0]</text>\n",
       "<text text-anchor=\"start\" x=\"75\" y=\"-244.07\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = setosa</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;1 -->\n",
       "<g id=\"edge1\" class=\"edge\">\n",
       "<title>0&#45;&gt;1</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M167.33,-352.3C160.39,-341.27 152.9,-329.38 145.91,-318.27\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"149.02,-316.63 140.73,-310.03 143.09,-320.36 149.02,-316.63\"/>\n",
       "<text text-anchor=\"middle\" x=\"134.38\" y=\"-327.71\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">True</text>\n",
       "</g>\n",
       "<!-- 2 -->\n",
       "<g id=\"node3\" class=\"node\">\n",
       "<title>2</title>\n",
       "<path fill=\"#ffffff\" stroke=\"black\" d=\"M335.75,-316.5C335.75,-316.5 206.5,-316.5 206.5,-316.5 200.5,-316.5 194.5,-310.5 194.5,-304.5 194.5,-304.5 194.5,-241.75 194.5,-241.75 194.5,-235.75 200.5,-229.75 206.5,-229.75 206.5,-229.75 335.75,-229.75 335.75,-229.75 341.75,-229.75 347.75,-235.75 347.75,-241.75 347.75,-241.75 347.75,-304.5 347.75,-304.5 347.75,-310.5 341.75,-316.5 335.75,-316.5\"/>\n",
       "<text text-anchor=\"start\" x=\"202.5\" y=\"-299.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal width (cm) ≤ 1.75</text>\n",
       "<text text-anchor=\"start\" x=\"243.38\" y=\"-283.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.5</text>\n",
       "<text text-anchor=\"start\" x=\"226.5\" y=\"-267.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 100</text>\n",
       "<text text-anchor=\"start\" x=\"217.12\" y=\"-251.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 50, 50]</text>\n",
       "<text text-anchor=\"start\" x=\"219\" y=\"-236.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 0&#45;&gt;2 -->\n",
       "<g id=\"edge2\" class=\"edge\">\n",
       "<title>0&#45;&gt;2</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M221.27,-352.3C226.62,-343.92 232.29,-335.03 237.81,-326.36\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"240.72,-328.31 243.15,-318 234.82,-324.55 240.72,-328.31\"/>\n",
       "<text text-anchor=\"middle\" x=\"249.36\" y=\"-335.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">False</text>\n",
       "</g>\n",
       "<!-- 3 -->\n",
       "<g id=\"node4\" class=\"node\">\n",
       "<title>3</title>\n",
       "<path fill=\"#4de88e\" stroke=\"black\" d=\"M249.38,-193.75C249.38,-193.75 114.88,-193.75 114.88,-193.75 108.88,-193.75 102.88,-187.75 102.88,-181.75 102.88,-181.75 102.88,-119 102.88,-119 102.88,-113 108.88,-107 114.88,-107 114.88,-107 249.38,-107 249.38,-107 255.38,-107 261.38,-113 261.38,-119 261.38,-119 261.38,-181.75 261.38,-181.75 261.38,-187.75 255.38,-193.75 249.38,-193.75\"/>\n",
       "<text text-anchor=\"start\" x=\"110.88\" y=\"-176.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.95</text>\n",
       "<text text-anchor=\"start\" x=\"146.88\" y=\"-160.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.168</text>\n",
       "<text text-anchor=\"start\" x=\"141.25\" y=\"-144.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 54</text>\n",
       "<text text-anchor=\"start\" x=\"131.88\" y=\"-129.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 49, 5]</text>\n",
       "<text text-anchor=\"start\" x=\"130\" y=\"-113.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;3 -->\n",
       "<g id=\"edge3\" class=\"edge\">\n",
       "<title>2&#45;&gt;3</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M239.75,-229.55C233.43,-220.99 226.73,-211.89 220.22,-203.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"223.17,-201.16 214.42,-195.19 217.53,-205.31 223.17,-201.16\"/>\n",
       "</g>\n",
       "<!-- 6 -->\n",
       "<g id=\"node7\" class=\"node\">\n",
       "<title>6</title>\n",
       "<path fill=\"#843de6\" stroke=\"black\" d=\"M426.38,-193.75C426.38,-193.75 291.88,-193.75 291.88,-193.75 285.88,-193.75 279.88,-187.75 279.88,-181.75 279.88,-181.75 279.88,-119 279.88,-119 279.88,-113 285.88,-107 291.88,-107 291.88,-107 426.38,-107 426.38,-107 432.38,-107 438.38,-113 438.38,-119 438.38,-119 438.38,-181.75 438.38,-181.75 438.38,-187.75 432.38,-193.75 426.38,-193.75\"/>\n",
       "<text text-anchor=\"start\" x=\"287.88\" y=\"-176.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">petal length (cm) ≤ 4.85</text>\n",
       "<text text-anchor=\"start\" x=\"323.88\" y=\"-160.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.043</text>\n",
       "<text text-anchor=\"start\" x=\"318.25\" y=\"-144.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 46</text>\n",
       "<text text-anchor=\"start\" x=\"308.88\" y=\"-129.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 45]</text>\n",
       "<text text-anchor=\"start\" x=\"311.12\" y=\"-113.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 2&#45;&gt;6 -->\n",
       "<g id=\"edge6\" class=\"edge\">\n",
       "<title>2&#45;&gt;6</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M302.15,-229.55C308.39,-220.99 315.02,-211.89 321.46,-203.06\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"324.13,-205.34 327.19,-195.19 318.47,-201.21 324.13,-205.34\"/>\n",
       "</g>\n",
       "<!-- 4 -->\n",
       "<g id=\"node5\" class=\"node\">\n",
       "<title>4</title>\n",
       "<path fill=\"#3de684\" stroke=\"black\" d=\"M108.25,-71C108.25,-71 12,-71 12,-71 6,-71 0,-65 0,-59 0,-59 0,-12 0,-12 0,-6 6,0 12,0 12,0 108.25,0 108.25,0 114.25,0 120.25,-6 120.25,-12 120.25,-12 120.25,-59 120.25,-59 120.25,-65 114.25,-71 108.25,-71\"/>\n",
       "<text text-anchor=\"start\" x=\"24.88\" y=\"-53.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.041</text>\n",
       "<text text-anchor=\"start\" x=\"19.25\" y=\"-37.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 48</text>\n",
       "<text text-anchor=\"start\" x=\"9.88\" y=\"-22.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 47, 1]</text>\n",
       "<text text-anchor=\"start\" x=\"8\" y=\"-6.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = versicolor</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;4 -->\n",
       "<g id=\"edge4\" class=\"edge\">\n",
       "<title>3&#45;&gt;4</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M136,-106.7C126.19,-97.62 115.83,-88.04 106.03,-78.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"108.41,-76.41 98.7,-72.19 103.66,-81.55 108.41,-76.41\"/>\n",
       "</g>\n",
       "<!-- 5 -->\n",
       "<g id=\"node6\" class=\"node\">\n",
       "<title>5</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M238.12,-71C238.12,-71 150.12,-71 150.12,-71 144.12,-71 138.12,-65 138.12,-59 138.12,-59 138.12,-12 138.12,-12 138.12,-6 144.12,0 150.12,0 150.12,0 238.12,0 238.12,0 244.12,0 250.12,-6 250.12,-12 250.12,-12 250.12,-59 250.12,-59 250.12,-65 244.12,-71 238.12,-71\"/>\n",
       "<text text-anchor=\"start\" x=\"158.88\" y=\"-53.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"157\" y=\"-37.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 6</text>\n",
       "<text text-anchor=\"start\" x=\"147.62\" y=\"-22.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 2, 4]</text>\n",
       "<text text-anchor=\"start\" x=\"146.12\" y=\"-6.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 3&#45;&gt;5 -->\n",
       "<g id=\"edge5\" class=\"edge\">\n",
       "<title>3&#45;&gt;5</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M186.66,-106.7C187.5,-98.82 188.38,-90.55 189.23,-82.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"192.7,-82.98 190.28,-72.66 185.74,-82.24 192.7,-82.98\"/>\n",
       "</g>\n",
       "<!-- 7 -->\n",
       "<g id=\"node8\" class=\"node\">\n",
       "<title>7</title>\n",
       "<path fill=\"#c09cf2\" stroke=\"black\" d=\"M391.12,-71C391.12,-71 303.12,-71 303.12,-71 297.12,-71 291.12,-65 291.12,-59 291.12,-59 291.12,-12 291.12,-12 291.12,-6 297.12,0 303.12,0 303.12,0 391.12,0 391.12,0 397.12,0 403.12,-6 403.12,-12 403.12,-12 403.12,-59 403.12,-59 403.12,-65 397.12,-71 391.12,-71\"/>\n",
       "<text text-anchor=\"start\" x=\"311.88\" y=\"-53.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.444</text>\n",
       "<text text-anchor=\"start\" x=\"310\" y=\"-37.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 3</text>\n",
       "<text text-anchor=\"start\" x=\"300.62\" y=\"-22.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 1, 2]</text>\n",
       "<text text-anchor=\"start\" x=\"299.12\" y=\"-6.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;7 -->\n",
       "<g id=\"edge7\" class=\"edge\">\n",
       "<title>6&#45;&gt;7</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M354.59,-106.7C353.75,-98.82 352.87,-90.55 352.02,-82.57\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"355.51,-82.24 350.97,-72.66 348.55,-82.98 355.51,-82.24\"/>\n",
       "</g>\n",
       "<!-- 8 -->\n",
       "<g id=\"node9\" class=\"node\">\n",
       "<title>8</title>\n",
       "<path fill=\"#8139e5\" stroke=\"black\" d=\"M525.38,-71C525.38,-71 432.88,-71 432.88,-71 426.88,-71 420.88,-65 420.88,-59 420.88,-59 420.88,-12 420.88,-12 420.88,-6 426.88,0 432.88,0 432.88,0 525.38,0 525.38,0 531.38,0 537.38,-6 537.38,-12 537.38,-12 537.38,-59 537.38,-59 537.38,-65 531.38,-71 525.38,-71\"/>\n",
       "<text text-anchor=\"start\" x=\"451.38\" y=\"-53.7\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">gini = 0.0</text>\n",
       "<text text-anchor=\"start\" x=\"438.25\" y=\"-37.95\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">samples = 43</text>\n",
       "<text text-anchor=\"start\" x=\"428.88\" y=\"-22.2\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">value = [0, 0, 43]</text>\n",
       "<text text-anchor=\"start\" x=\"431.12\" y=\"-6.45\" font-family=\"Helvetica,sans-Serif\" font-size=\"14.00\">class = virginica</text>\n",
       "</g>\n",
       "<!-- 6&#45;&gt;8 -->\n",
       "<g id=\"edge8\" class=\"edge\">\n",
       "<title>6&#45;&gt;8</title>\n",
       "<path fill=\"none\" stroke=\"black\" d=\"M404.49,-106.7C414.14,-97.62 424.33,-88.04 433.97,-78.97\"/>\n",
       "<polygon fill=\"black\" stroke=\"black\" points=\"436.29,-81.6 441.18,-72.19 431.5,-76.5 436.29,-81.6\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>\n"
      ],
      "text/plain": [
       "<graphviz.sources.Source at 0x2cdbff853d0>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "import graphviz\n",
    "\n",
    "iris = load_iris()\n",
    "X, y = iris.data, iris.target\n",
    "\n",
    "clf = DecisionTreeClassifier(criterion=\"gini\", max_depth=3, random_state=42)\n",
    "clf.fit(X, y)\n",
    "\n",
    "dot_data = export_graphviz(\n",
    "    clf, out_file=None, feature_names=iris.feature_names,  \n",
    "    class_names=iris.target_names, filled=True, rounded=True, special_characters=True\n",
    ")\n",
    "\n",
    "graph = graphviz.Source(dot_data)\n",
    "graph.render(\"decision_tree\")  \n",
    "graph.view()  \n",
    "\n",
    "graph"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a23d2f9-eb3a-43c5-933c-1e08dbb760df",
   "metadata": {},
   "source": [
    "### 20) Train a Decision Tree Classifier and visualize the tree using Graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c8ebfdd6-45ac-4771-9414-8325e5a6fcbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'decision_tree.pdf'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "dot_data = tree.export_graphviz(clf, out_file=None, \n",
    "                                feature_names=data.feature_names,  \n",
    "                                class_names=data.target_names,  \n",
    "                                filled=True, rounded=True,  \n",
    "                                special_characters=True)  \n",
    "graph = graphviz.Source(dot_data)  \n",
    "graph.render(\"decision_tree\")\n",
    "graph.view()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "392ea8a8-785d-4ea2-9d4e-db4f0aa36f78",
   "metadata": {},
   "source": [
    "### 21) Train a Decision Tree Classifier with a maximum depth of 3 and compare its accuracy with a fully grown tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f243de80-375c-4de8-9914-7ac7f6471321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with max_depth=3: 1.0000\n",
      "Accuracy of fully grown Decision Tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "clf_depth_3 = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
    "clf_depth_3.fit(X_train, y_train)\n",
    "\n",
    "clf_full = DecisionTreeClassifier(random_state=42)\n",
    "clf_full.fit(X_train, y_train)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "y_pred_depth_3 = clf_depth_3.predict(X_test)\n",
    "y_pred_full = clf_full.predict(X_test)\n",
    "\n",
    "accuracy_depth_3 = accuracy_score(y_test, y_pred_depth_3)\n",
    "accuracy_full = accuracy_score(y_test, y_pred_full)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree with max_depth=3: {accuracy_depth_3:.4f}\")\n",
    "print(f\"Accuracy of fully grown Decision Tree: {accuracy_full:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7900ecdc-c48d-4b36-94ac-7d84229a5e57",
   "metadata": {},
   "source": [
    "### 22) Train a Decision Tree Classifier using min_samples_split=5 and compare its accuracy with a default tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b154ab2b-ce65-422a-92a1-a2a0c1198dc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with min_samples_split=5: 1.0000\n",
      "Accuracy of default Decision Tree: 1.0000\n"
     ]
    }
   ],
   "source": [
    "clf_min_samples_split = DecisionTreeClassifier(min_samples_split=5, random_state=42)\n",
    "clf_min_samples_split.fit(X_train, y_train)\n",
    "\n",
    "clf_default = DecisionTreeClassifier(random_state=42)\n",
    "clf_default.fit(X_train, y_train)\n",
    "\n",
    "y_pred_min_samples_split = clf_min_samples_split.predict(X_test)\n",
    "y_pred_default = clf_default.predict(X_test)\n",
    "\n",
    "accuracy_min_samples_split = accuracy_score(y_test, y_pred_min_samples_split)\n",
    "accuracy_default = accuracy_score(y_test, y_pred_default)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree with min_samples_split=5: {accuracy_min_samples_split:.4f}\")\n",
    "print(f\"Accuracy of default Decision Tree: {accuracy_default:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e3fe1e-0fa8-4706-a19c-9fa7fac60aa3",
   "metadata": {},
   "source": [
    "### 23) Apply feature scaling before training a Decision Tree Classifier and compare its accuracy with unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dbc09c34-8bef-4ad7-b2c5-ae3206e1d71b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with scaled data: 1.0000\n",
      "Accuracy of Decision Tree with unscaled data: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "clf_scaled = DecisionTreeClassifier(random_state=42)\n",
    "clf_scaled.fit(X_train_scaled, y_train)\n",
    "\n",
    "clf_unscaled = DecisionTreeClassifier(random_state=42)\n",
    "clf_unscaled.fit(X_train, y_train)\n",
    "\n",
    "y_pred_scaled = clf_scaled.predict(X_test_scaled)\n",
    "y_pred_unscaled = clf_unscaled.predict(X_test)\n",
    "\n",
    "accuracy_scaled = accuracy_score(y_test, y_pred_scaled)\n",
    "accuracy_unscaled = accuracy_score(y_test, y_pred_unscaled)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree with scaled data: {accuracy_scaled:.4f}\")\n",
    "print(f\"Accuracy of Decision Tree with unscaled data: {accuracy_unscaled:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16154e72-559a-49f4-af35-b8ea6019cad0",
   "metadata": {},
   "source": [
    "### 24) Train a Decision Tree Classifier using One-vs-Rest (OvR) strategy for multiclass classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0bce147-dc34-406a-a357-cc36fc74446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of Decision Tree with One-vs-Rest strategy: 1.0000\n"
     ]
    }
   ],
   "source": [
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "\n",
    "clf_ovr = OneVsRestClassifier(DecisionTreeClassifier(random_state=42))\n",
    "clf_ovr.fit(X_train, y_train)\n",
    "\n",
    "y_pred_ovr = clf_ovr.predict(X_test)\n",
    "accuracy_ovr = accuracy_score(y_test, y_pred_ovr)\n",
    "\n",
    "print(f\"Accuracy of Decision Tree with One-vs-Rest strategy: {accuracy_ovr:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809c23d5-d360-4d52-a5c0-a74e5cb1fbdb",
   "metadata": {},
   "source": [
    "### 25) Train a Decision Tree Classifier and display the feature importance scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "da973ad3-2cff-440e-a86d-1e12b85ca77e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature importance scores:\n",
      "sepal length (cm): 0.0000\n",
      "sepal width (cm): 0.0191\n",
      "petal length (cm): 0.8933\n",
      "petal width (cm): 0.0876\n"
     ]
    }
   ],
   "source": [
    "clf_importance = DecisionTreeClassifier(random_state=42)\n",
    "clf_importance.fit(X_train, y_train)\n",
    "\n",
    "feature_importance = clf_importance.feature_importances_\n",
    "print(\"Feature importance scores:\")\n",
    "for feature, importance in zip(data.feature_names, feature_importance):\n",
    "    print(f\"{feature}: {importance:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71acd052-deba-437d-afbb-08f0659425d9",
   "metadata": {},
   "source": [
    "### 26) Train a Decision Tree Regressor with max_depth=5 and compare its performance with an unrestricted tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "6637c5be-b6ec-494b-a79e-e4ead1525e98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Decision Tree Regressor with max_depth=5: 0.5211\n",
      "MSE for unrestricted Decision Tree Regressor: 0.5280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "regressor_depth_5 = DecisionTreeRegressor(max_depth=5, random_state=42)\n",
    "regressor_depth_5.fit(X_train, y_train)\n",
    "\n",
    "regressor_unrestricted = DecisionTreeRegressor(random_state=42)\n",
    "regressor_unrestricted.fit(X_train, y_train)\n",
    "\n",
    "y_pred_depth_5 = regressor_depth_5.predict(X_test)\n",
    "y_pred_unrestricted = regressor_unrestricted.predict(X_test)\n",
    "\n",
    "mse_depth_5 = mean_squared_error(y_test, y_pred_depth_5)\n",
    "mse_unrestricted = mean_squared_error(y_test, y_pred_unrestricted)\n",
    "\n",
    "print(f\"MSE for Decision Tree Regressor with max_depth=5: {mse_depth_5:.4f}\")\n",
    "print(f\"MSE for unrestricted Decision Tree Regressor: {mse_unrestricted:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94f0e822-478f-4061-b0ad-d963fd4c3938",
   "metadata": {},
   "source": [
    "### 27) Train a Decision Tree Classifier, apply Cost Complexity Pruning (CCP), and visualize its effect on accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b43fcc87-78c4-474a-8b54-7f37f2fc910a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Decision Tree Regressor without pruning: 0.5280\n",
      "MSE for pruned Decision Tree Regressor: 0.9195\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "path = regressor.cost_complexity_pruning_path(X_train, y_train)\n",
    "ccp_alphas = path.ccp_alphas\n",
    "\n",
    "regressor_pruned = DecisionTreeRegressor(random_state=42, ccp_alpha=ccp_alphas[-2])\n",
    "regressor_pruned.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "y_pred_pruned = regressor_pruned.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_pruned = mean_squared_error(y_test, y_pred_pruned)\n",
    "\n",
    "print(f\"MSE for Decision Tree Regressor without pruning: {mse:.4f}\")\n",
    "print(f\"MSE for pruned Decision Tree Regressor: {mse_pruned:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b07817-eb82-4b2f-931d-de7b4775ccfb",
   "metadata": {},
   "source": [
    "### 28) Train a Decision Tree Classifier and evaluate its performance using Precision, Recall, and F1-Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1fbff8ef-0d43-4a09-a85b-f3b73a996aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Decision Tree Regressor: 0.5280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE for Decision Tree Regressor: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbbf01ef-508d-416d-977c-8ca0604d0d70",
   "metadata": {},
   "source": [
    "### 29) Train a Decision Tree Classifier and visualize the confusion matrix using seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "9959335d-75a8-4ca5-96db-a499991c0676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MSE for Decision Tree Regressor: 0.5280\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "regressor.fit(X_train, y_train) \n",
    "\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(f\"MSE for Decision Tree Regressor: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b25ba05-f3a6-4ced-ab36-b2d7919a2879",
   "metadata": {},
   "source": [
    "### 30) Train a Decision Tree Classifier and use GridSearchCV to find the optimal values for max_depth and min_samples_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "95a004e4-5d9a-438c-b93a-ab74ad3073de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'max_depth': 10, 'min_samples_split': 10}\n",
      "Best cross-validation score: -0.4306\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 10, None],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "regressor = DecisionTreeRegressor(random_state=42)\n",
    "\n",
    "grid_search = GridSearchCV(estimator=regressor, param_grid=param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best cross-validation score: {grid_search.best_score_:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ac94c7-3e1a-47bd-a79a-4ec3dc6d329e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

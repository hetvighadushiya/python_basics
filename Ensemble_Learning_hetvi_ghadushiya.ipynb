{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8f6cc47-b0a7-4366-91ba-a92874675297",
   "metadata": {},
   "source": [
    "# Theorical Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45b9950b-aba6-44c8-9cc8-38dbad770465",
   "metadata": {},
   "source": [
    "### 1. What is Unsupervised Learning in the Context of Machine Learning?\n",
    "\n",
    "Unsupervised learning is a type of machine learning where the algorithm is given data without labeled responses. The goal is to find hidden patterns or intrinsic structures in the input data. It is commonly used for clustering, dimensionality reduction, and association rule learning.\n",
    "\n",
    "Examples include:\n",
    "- Clustering (e.g., K-Means, DBSCAN)\n",
    "- Dimensionality Reduction (e.g., PCA, t-SNE)\n",
    "\n",
    "### 2. How Does K-Means Clustering Algorithm Work?\n",
    "\n",
    "K-Means is a partitioning clustering algorithm that divides the dataset into **K clusters**. It works as follows:\n",
    "\n",
    "1. Initialize K centroids randomly.\n",
    "2. Assign each data point to the nearest centroid based on Euclidean distance.\n",
    "3. Recalculate the centroids as the mean of the points in each cluster.\n",
    "4. Repeat steps 2 and 3 until the centroids no longer change significantly or a maximum number of iterations is reached.\n",
    "\n",
    "The goal is to minimize the **inertia**, which is the sum of squared distances between data points and their respective cluster centers.\n",
    "\n",
    "### 3. Explain the Concept of a Dendrogram in Hierarchical Clustering\n",
    "\n",
    "A dendrogram is a tree-like diagram used to represent the arrangement of the clusters produced by hierarchical clustering. It shows:\n",
    "- The order in which clusters are merged or split.\n",
    "- The distance at which each merge occurs.\n",
    "\n",
    "By cutting the dendrogram at a specific height, we can select the number of clusters desired. It helps in visualizing the hierarchical relationships between clusters.\n",
    "\n",
    "### 4. What is the Main Difference Between K-Means and Hierarchical Clustering?\n",
    "\n",
    "| Feature | K-Means | Hierarchical Clustering |\n",
    "|--------|---------|--------------------------|\n",
    "| Type | Partitional | Hierarchical |\n",
    "| Cluster number | Must be specified before clustering | Not required beforehand |\n",
    "| Output | Flat clusters | Tree-like structure (dendrogram) |\n",
    "| Scalability | Efficient for large datasets | Not suitable for very large datasets |\n",
    "| Reproducibility | Depends on random initialization | Deterministic |\n",
    "\n",
    "The main difference is that K-Means divides the data into K distinct, non-overlapping clusters, while hierarchical clustering builds a hierarchy of clusters without requiring the number of clusters upfront.\n",
    "\n",
    "### 5. What Are the Advantages of DBSCAN Over K-Means?\n",
    "\n",
    "DBSCAN (Density-Based Spatial Clustering of Applications with Noise) offers several advantages over K-Means:\n",
    "\n",
    "- **No need to specify number of clusters (K)**.\n",
    "- Can **detect arbitrary shaped clusters**, not just spherical.\n",
    "- Can **identify outliers** as noise.\n",
    "- **Robust to noise** and outliers.\n",
    "- Works well with clusters of **different sizes and densities**.\n",
    "\n",
    "However, it may struggle with very high-dimensional data or when the density varies greatly between clusters.\n",
    "\n",
    "### 6. Explain the working principle of a Bagging Classifier\n",
    "\n",
    "Bagging (Bootstrap Aggregating) is an ensemble learning method that builds multiple versions of a predictor and uses these to get an aggregated prediction. For classification tasks, it combines the predictions of base estimators (often decision trees) using majority voting. Each model is trained on a random sample of the training data (with replacement), which helps to reduce variance and avoid overfitting.\n",
    "\n",
    "\n",
    "### 7. How do you evaluate a Bagging Classifier’s performance?\n",
    "\n",
    "To evaluate a Bagging Classifier, we use common classification metrics such as:\n",
    "\n",
    "- **Accuracy**: The ratio of correct predictions to total predictions.\n",
    "- **Precision, Recall, and F1-Score**: Useful for imbalanced datasets.\n",
    "- **Confusion Matrix**: To understand how predictions are distributed across classes.\n",
    "- **ROC-AUC Score**: For binary classification, helps to visualize performance at various thresholds.\n",
    "- **Cross-validation**: Provides a more robust estimate of model performance.\n",
    "\n",
    "\n",
    "### 8. How does a Bagging Regressor work?\n",
    "\n",
    "Bagging Regressor works on the same principle as Bagging Classifier but for regression tasks. It:\n",
    "\n",
    "- Trains multiple regressors (like decision trees) on different bootstrap samples.\n",
    "- Aggregates the predictions by taking the **average** of individual model outputs.\n",
    "- Helps reduce variance and improve prediction stability.\n",
    "\n",
    "\n",
    "### 9. What is the main advantage of ensemble techniques?\n",
    "\n",
    "The main advantage of ensemble techniques is improved **accuracy** and **robustness**. By combining multiple models:\n",
    "\n",
    "- It reduces the likelihood of overfitting.\n",
    "- It compensates for weaknesses in individual models.\n",
    "- It generally improves generalization performance on unseen data.\n",
    "\n",
    "Ensembles often outperform individual models in complex real-world tasks.\n",
    "\n",
    "\n",
    "### 10. What is the main challenge of ensemble methods?\n",
    "\n",
    "The main challenges of ensemble methods include:\n",
    "\n",
    "- **Increased complexity**: Difficult to interpret and debug.\n",
    "- **Computational cost**: Requires more memory and training time.\n",
    "- **Tuning**: More parameters and models to tune.\n",
    "- **Reduced interpretability**: It's harder to explain the final prediction made by an ensemble.\n",
    "\n",
    "Despite these challenges, the performance gains often justify the added complexity.\n",
    "\n",
    "### 11. Explain the key idea behind ensemble techniques\n",
    "\n",
    "The key idea behind ensemble techniques is to **combine the predictions of multiple models** to produce a single, stronger model. By leveraging the diversity and strength of individual models, ensemble methods help to improve **accuracy**, **robustness**, and **generalization**. The combined output is often more accurate and reliable than that of any single model, especially in complex or noisy datasets.\n",
    "\n",
    "\n",
    "### 12. What is a Random Forest Classifier?\n",
    "\n",
    "A **Random Forest Classifier** is an ensemble learning method that builds a large number of decision trees during training. Each tree is trained on a random subset of the data and features. When making a prediction, the random forest takes a **majority vote** across all the trees.\n",
    "\n",
    "It combines the **power of bagging** (bootstrapping + aggregation) with **random feature selection**, which helps reduce **overfitting** and improve **performance** on unseen data.\n",
    "\n",
    "\n",
    "### 13. What are the main types of ensemble techniques?\n",
    "\n",
    "The main types of ensemble techniques are:\n",
    "\n",
    "1. **Bagging (Bootstrap Aggregating)**  \n",
    "   - Builds multiple models independently using bootstrapped datasets.\n",
    "   - Example: Random Forest.\n",
    "\n",
    "2. **Boosting**  \n",
    "   - Builds models sequentially, each trying to correct the errors of the previous one.\n",
    "   - Examples: AdaBoost, Gradient Boosting, XGBoost.\n",
    "\n",
    "3. **Stacking**  \n",
    "   - Combines multiple models (called base learners) and uses another model (called a meta-learner) to make final predictions.\n",
    "\n",
    "Each type focuses on improving model performance by addressing different kinds of errors (bias or variance).\n",
    "\n",
    "\n",
    "### 14. What is ensemble learning in machine learning?\n",
    "\n",
    "**Ensemble learning** is a machine learning paradigm where multiple models (often called \"weak learners\") are trained to solve the same problem and their predictions are combined. The goal is to achieve better performance than any single model alone.\n",
    "\n",
    "It helps to:\n",
    "\n",
    "- Reduce overfitting (variance)\n",
    "- Improve accuracy\n",
    "- Make predictions more robust\n",
    "\n",
    "Ensemble methods are widely used in many real-world applications and are often top performers in machine learning competitions.\n",
    "\n",
    "\n",
    "### 15. When should we avoid using ensemble methods?\n",
    "\n",
    "We should avoid using ensemble methods when:\n",
    "\n",
    "- **Model interpretability is important**: Ensembles are complex and hard to explain.\n",
    "- **Data is very small**: Ensemble methods can overfit when there’s not enough data.\n",
    "- **Real-time performance is critical**: They can be computationally expensive in training and prediction.\n",
    "- **Simplicity is preferred**: In cases where a single simple model performs sufficiently well, ensembles may not be worth the added complexity.\n",
    "\n",
    "In such situations, simpler models like logistic regression or single decision trees might be more suitable.\n",
    "\n",
    "### 16. How does Bagging help in reducing overfitting?\n",
    "\n",
    "Bagging (Bootstrap Aggregating) helps in reducing overfitting by training multiple models on different subsets of the data and averaging their predictions. Each subset is drawn using **bootstrap sampling** (sampling with replacement), which introduces **diversity** among models. Since overfitting typically arises from a model being too closely fitted to a single dataset, combining predictions from many varied models **reduces variance** and increases robustness.\n",
    "\n",
    "\n",
    "### 17. Why is Random Forest better than a single Decision Tree?\n",
    "\n",
    "Random Forest is better than a single Decision Tree because:\n",
    "\n",
    "- It reduces **overfitting** by averaging multiple trees.\n",
    "- It improves **generalization** to unseen data.\n",
    "- It uses **random feature selection**, which introduces additional diversity among trees.\n",
    "- It’s more **stable** and less sensitive to noise in the data.\n",
    "\n",
    "While a single decision tree may capture noise and make inconsistent predictions, Random Forest mitigates this by combining multiple, less correlated models.\n",
    "\n",
    "\n",
    "### 18. What is the role of bootstrap sampling in Bagging?\n",
    "\n",
    "Bootstrap sampling is the process of creating multiple datasets by **randomly sampling with replacement** from the original dataset. Each model in the bagging ensemble is trained on a different bootstrap sample. This helps:\n",
    "\n",
    "- Introduce **variation** in the training process.\n",
    "- Ensure that each model sees a slightly different version of the data.\n",
    "- **Reduce variance** by combining diverse models.\n",
    "\n",
    "It is the core mechanism that allows Bagging to be effective.\n",
    "\n",
    "\n",
    "### 19. What are some real-world applications of ensemble techniques?\n",
    "\n",
    "Ensemble techniques are used in a wide variety of real-world applications, including:\n",
    "\n",
    "- **Spam detection**: Combining classifiers to filter spam emails.\n",
    "- **Fraud detection**: Ensemble models like Random Forests are used to identify suspicious transactions.\n",
    "- **Medical diagnosis**: Used to improve accuracy in predicting diseases from diagnostic data.\n",
    "- **Recommendation systems**: Ensembles can combine multiple models for better personalization.\n",
    "- **Finance and credit scoring**: Predicting loan defaults or credit risks.\n",
    "\n",
    "They are particularly useful in domains where **accuracy is critical**.\n",
    "\n",
    "### 20. What is the difference between Bagging and Boosting?\n",
    "\n",
    "| Feature              | Bagging                             | Boosting                              |\n",
    "|----------------------|--------------------------------------|----------------------------------------|\n",
    "| Training Strategy    | Models trained **in parallel**       | Models trained **sequentially**        |\n",
    "| Data Sampling        | Bootstrap sampling (random subsets)  | Focus on errors made by previous models |\n",
    "| Goal                 | Reduce **variance**                  | Reduce **bias and variance**           |\n",
    "| Model Combination    | **Averaging** (regression) or majority voting (classification) | **Weighted** voting or sum             |\n",
    "| Example Algorithms   | Random Forest                        | AdaBoost, Gradient Boosting, XGBoost   |\n",
    "\n",
    "In short, **Bagging** reduces overfitting by creating independent models, while **Boosting** builds a strong model by correcting the errors of previous models.\n",
    "                                                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92d23458-5841-4525-808e-9e4e51e2075a",
   "metadata": {},
   "source": [
    "# Practical Questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "362424be-feaa-4b58-b4bb-dbcf9222035d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Classifier Accuracy: 1.0\n"
     ]
    }
   ],
   "source": [
    "# 21. Train a Bagging Classifier using Decision Trees on a sample dataset and print model accuracy\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "iris = load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.3, random_state=42)\n",
    "\n",
    "bagging_clf = BaggingClassifier(DecisionTreeClassifier(), n_estimators=10, random_state=0)\n",
    "bagging_clf.fit(X_train, y_train)\n",
    "y_pred = bagging_clf.predict(X_test)\n",
    "print(\"Bagging Classifier Accuracy:\", accuracy_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e136702-81a3-4f33-ac18-3e3f1eafd3a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor MSE: 0.27826689951710337\n"
     ]
    }
   ],
   "source": [
    "# 22. Train a Bagging Regressor using Decision Trees and evaluate using Mean Squared Error (MSE)\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X, y = data.data, data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "bagging_reg = BaggingRegressor(DecisionTreeRegressor(), n_estimators=10, random_state=0)\n",
    "\n",
    "bagging_reg.fit(X_train, y_train)\n",
    "y_pred = bagging_reg.predict(X_test)\n",
    "\n",
    "print(\"Bagging Regressor MSE:\", mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f41a3dae-b8e4-4913-bed0-7672b3042ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Importances:\n",
      " mean concave points        0.141934\n",
      "worst concave points       0.127136\n",
      "worst area                 0.118217\n",
      "mean concavity             0.080557\n",
      "worst radius               0.077975\n",
      "worst perimeter            0.074292\n",
      "mean perimeter             0.060092\n",
      "mean area                  0.053810\n",
      "worst concavity            0.041080\n",
      "mean radius                0.032312\n",
      "area error                 0.029538\n",
      "worst texture              0.018786\n",
      "worst compactness          0.017539\n",
      "radius error               0.016435\n",
      "worst symmetry             0.012929\n",
      "perimeter error            0.011770\n",
      "worst smoothness           0.011769\n",
      "mean texture               0.011064\n",
      "mean compactness           0.009216\n",
      "fractal dimension error    0.007135\n",
      "worst fractal dimension    0.006924\n",
      "mean smoothness            0.006223\n",
      "smoothness error           0.005881\n",
      "concavity error            0.005816\n",
      "compactness error          0.004596\n",
      "symmetry error             0.004001\n",
      "concave points error       0.003382\n",
      "mean symmetry              0.003278\n",
      "texture error              0.003172\n",
      "mean fractal dimension     0.003140\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# 23. Train a Random Forest Classifier on the Breast Cancer dataset and print feature importance scores\n",
    "\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import pandas as pd\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X_train, X_test, y_train, y_test = train_test_split(data.data, data.target, test_size=0.3, random_state=42)\n",
    "\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "rf_clf.fit(X_train, y_train)\n",
    "\n",
    "# Display feature importances\n",
    "feature_importances = pd.Series(rf_clf.feature_importances_, index=data.feature_names)\n",
    "print(\"Feature Importances:\\n\", feature_importances.sort_values(ascending=False))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f3441ad-5ce6-4937-b5de-2f110f7959f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest Regressor MSE: 0.0345280701754386\n",
      "Decision Tree Regressor MSE: 0.07602339181286549\n"
     ]
    }
   ],
   "source": [
    "# 24. Train a Random Forest Regressor and compare its performance with a single Decision Tree\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Use same regression data from Q22\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=0)\n",
    "tree_reg = DecisionTreeRegressor(random_state=0)\n",
    "\n",
    "rf_reg.fit(X_train, y_train)\n",
    "tree_reg.fit(X_train, y_train)\n",
    "\n",
    "rf_pred = rf_reg.predict(X_test)\n",
    "tree_pred = tree_reg.predict(X_test)\n",
    "\n",
    "print(\"Random Forest Regressor MSE:\", mean_squared_error(y_test, rf_pred))\n",
    "print(\"Decision Tree Regressor MSE:\", mean_squared_error(y_test, tree_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a530f284-ad3b-4935-8cc0-521c98cc8c59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OOB Score: 0.9547738693467337\n"
     ]
    }
   ],
   "source": [
    "# 25. Compute the Out-of-Bag (OOB) Score for a Random Forest Classifier\n",
    "\n",
    "oob_rf = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\n",
    "oob_rf.fit(X_train, y_train)\n",
    "print(\"OOB Score:\", oob_rf.oob_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06a9b218-758a-4dbd-bb40-799fbb849127",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 26. Train a Bagging Classifier using SVM as a base estimator and print accuracy\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "bagging_svm = BaggingClassifier(estimator=SVC(), n_estimators=10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffa4d83e-9b60-485b-bb8d-6ac6bb479a3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest with 10 trees: Accuracy = 1.0000\n",
      "Random Forest with 50 trees: Accuracy = 1.0000\n",
      "Random Forest with 100 trees: Accuracy = 1.0000\n",
      "Random Forest with 200 trees: Accuracy = 1.0000\n"
     ]
    }
   ],
   "source": [
    "# 27. Train a Random Forest Classifier with different numbers of trees and compare accuracy\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "data = load_iris()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "tree_nums = [10, 50, 100, 200]\n",
    "for n in tree_nums:\n",
    "    rf = RandomForestClassifier(n_estimators=n, random_state=42)\n",
    "    rf.fit(X_train, y_train)\n",
    "    y_pred = rf.predict(X_test)\n",
    "    print(f\"Random Forest with {n} trees: Accuracy = {accuracy_score(y_test, y_pred):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13a8a05d-d706-43ec-bf3e-6a1459592932",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 28. Train a Bagging Classifier using Logistic Regression as a base estimator and print AUC score\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing  # Use California housing dataset\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], \n",
    "    'max_depth': [10, 20, 30],     \n",
    "    'min_samples_split': [2, 5, 10]  \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')  # Use negative MSE for regression\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Mean Squared Error:\", -grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ce58e37b-f651-416f-82f5-6296ea01c0ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Feature  Importance\n",
      "0  Feature 0    0.525996\n",
      "5  Feature 5    0.138238\n",
      "7  Feature 7    0.086133\n",
      "6  Feature 6    0.086099\n",
      "1  Feature 1    0.054663\n",
      "2  Feature 2    0.047174\n",
      "4  Feature 4    0.031724\n",
      "3  Feature 3    0.029973\n"
     ]
    }
   ],
   "source": [
    "# 29. Train a Random Forest Regressor and analyze feature importance scores\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_reg.fit(X_train, y_train)\n",
    "importances = rf_reg.feature_importances_\n",
    "features = X_train.columns if hasattr(X_train, 'columns') else [f'Feature {i}' for i in range(X_train.shape[1])]\n",
    "importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "importance_df = importance_df.sort_values(by='Importance', ascending=False)\n",
    "print(importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b200d01-ae2c-447c-b980-0fb214a66ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 30. Train an ensemble model using both Bagging and Random Forest and compare accuracy\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing  # Alternative dataset\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')  # Use 'neg_mean_squared_error' for regression\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", -grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9acea28f-5391-4b18-a6d1-77f5ca480c0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 31. Train a Random Forest Classifier and tune hyperparameters using GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing  # Using California housing dataset for regression\n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [10, 20, 30],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')  # Use 'neg_mean_squared_error' for regression\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best MSE:\", -grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c97435ae-c7dd-41dc-a5b5-a939c9fdf97c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bagging Regressor with 10 estimators: MSE = 0.2863\n",
      "Bagging Regressor with 50 estimators: MSE = 0.2579\n",
      "Bagging Regressor with 100 estimators: MSE = 0.2569\n",
      "Bagging Regressor with 200 estimators: MSE = 0.2542\n"
     ]
    }
   ],
   "source": [
    "# 32. Train a Bagging Regressor with different numbers of base estimators and compare performance\n",
    "\n",
    "from sklearn.ensemble import BaggingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "estimators = [10, 50, 100, 200]\n",
    "for n in estimators:\n",
    "    bag_reg = BaggingRegressor(n_estimators=n, random_state=42)\n",
    "    bag_reg.fit(X_train, y_train)\n",
    "    y_pred = bag_reg.predict(X_test)\n",
    "    print(f\"Bagging Regressor with {n} estimators: MSE = {mean_squared_error(y_test, y_pred):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49928b28-2fb5-4aa6-bc50-e18b27c4b3ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 33. Train a Random Forest Classifier and analyze misclassified samples\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import fetch_california_housing \n",
    "\n",
    "data = fetch_california_housing()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200], \n",
    "    'max_depth': [10, 20, 30],  \n",
    "    'min_samples_split': [2, 5, 10] \n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=5, scoring='neg_mean_squared_error')  \n",
    "\n",
    "print(\"Best Parameters:\", grid_search.best_params_)\n",
    "print(\"Best Cross-Validation Mean Squared Error:\", -grid_search.best_score_)\n",
    "\n",
    "y_pred = grid_search.best_estimator_.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test Mean Squared Error: {mse:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8056c580-3dfb-41d9-8b0c-5ca31f68e08b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
